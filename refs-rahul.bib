@inproceedings{sarro2016multi,
title={Multi-objective Software Effort Estimation},
author={Sarro, Federica and Petrozziello, Alessio and Harman, Mark},
booktitle={38th International Conference on Software Engineering (ICSE'16)},
number={to appear},
pages={},
year={2016},
organization={ACM}
}
@inproceedings{Choetkiertikul15,
title={Predicting Delays in Software Projects Using Networked Classification (T)},
author={Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},
booktitle={Automated Software Engineering (ASE), 2015 30th IEEE/ACM International Conference on},
pages={353--364},
year={2015},
organization={IEEE}
}
@inproceedings{Rahman:2014,
title={Comparing static bug finders and statistical prediction},
author={Rahman, Foyzur and Khatri, Sameer and Barr, Earl T and Devanbu, Premkumar},
booktitle={Proceedings of the 36th International Conference on Software Engineering},
pages={424--434},
year={2014},
organization={ACM}
}
@book{Ca09,
  title={Dataset shift in machine learning},
  author={Quionero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D},
  year={2009},
  publisher={The MIT Press}
}
@ARTICLE{Ha06,
   author = {{Hand}, D.~J.},
    title = "{Classifier Technology and the Illusion of Progress}",
  journal = {ArXiv Mathematics e-prints},
   eprint = {math/0606441},
 keywords = {Mathematics - Statistics},
     year = 2006,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2006math......6441H},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@incollection{St09,
  doi = {10.7551/mitpress/9780262170055.003.0001},
  url = {https://doi.org/10.7551/mitpress/9780262170055.003.0001},
  year  = {2008},
  month = {dec},
  publisher = {The {MIT} Press},
  pages = {2--28},
  author = {Storkey, Amos},
  title = {When Training and Test Sets Are Different: Characterizing Learning Transfer},
  booktitle = {Dataset Shift in Machine Learning}
}
@article{Shin13,
title={Can traditional fault prediction models be used for vulnerability prediction?},
author={Shin, Yonghee and Williams, Laurie},
journal={Empirical Software Engineering},
volume={18},
number={1},
pages={25--59},
year={2013},
publisher={Springer US}
}
@article{vaux2012replicates,
title={Replicates and repeats—what is the difference and is it significant?},
author={Vaux, David L and Fidler, Fiona and Cumming, Geoff},
journal={EMBO reports},
volume={13},
number={4},
pages={291--296},
year={2012},
publisher={EMBO Press}
}
@book{cocomo,
title={Software engineering economics},
author={Boehm, Barry W and others},
volume={197},
year={1981},
publisher={Prentice-hall Englewood Cliffs (NJ)}
}
@Article{Menzies2016,
author="Menzies, Tim
and Yang, Ye
and Mathew, George
and Boehm, Barry
and Hihn, Jairus",
title="Negative results for software effort estimation",
journal="Empirical Software Engineering",
year="2016",
pages="1--26",
abstract="More than half the literature on software effort estimation (SEE) focuses on comparisons of new estimation methods. Surprisingly, there are no studies comparing state of the art latest methods with decades-old approaches. Accordingly, this paper takes five steps to check if new SEE methods generated better estimates than older methods. Firstly, collect effort estimation methods ranging from ``classical'' COCOMO (parametric estimation over a pre-determined set of attributes) to ``modern'' (reasoning via analogy using spectral-based clustering plus instance and feature selection, and a recent ``baseline method'' proposed in ACM Transactions on Software Engineering). Secondly, catalog the list of objections that lead to the development of post-COCOMO estimation methods. Thirdly, characterize each of those objections as a comparison between newer and older estimation methods. Fourthly, using four COCOMO-style data sets (from 1991, 2000, 2005, 2010) and run those comparisons experiments. Fifthly, compare the performance of the different estimators using a Scott-Knott procedure using (i) the A12 effect size to rule out ``small'' differences and (ii) a 99 {\%} confident bootstrap procedure to check for statistically different groupings of treatments. The major negative result of this paper is that for the COCOMO data sets, nothing we studied did any better than Boehms original procedure. Hence, we conclude that when COCOMO-style attributes are available, we strongly recommend (i) using that data and (ii) use COCOMO to generate predictions. We say this since the experiments of this paper show that, at least for effort estimation, how data is collected is more important than what learner is applied to that data.",
issn="1573-7616",
doi="10.1007/s10664-016-9472-2",
url="http://dx.doi.org/10.1007/s10664-016-9472-2"
}
@INPROCEEDINGS{Rahman:2013,
author={F. Rahman and P. Devanbu},
booktitle={2013 35th International Conference on Software Engineering (ICSE)},
title={How, and why, process metrics are better},
year={2013},
pages={432-441},
keywords={software metrics;software performance evaluation;software quality;statistical analysis;code metrics;defect prediction;defect-prone files;model deployment;performance;portability;process metrics;quality-assurance efforts;software metrics;stability;statistical tools;Complexity theory;Measurement;Object oriented modeling;Predictive models;Software;Support vector machines;Training},
doi={10.1109/ICSE.2013.6606589},
ISSN={0270-5257},
month={May},}
@article{me11f,
title =	 "Learning Better Inspection Optimization Policies",
author =	 "M. Lumpe and R. Vasa and T. Menzies and R. Rush and
R. Turhan",
class =	 "hJ",
volume =	 21,
number =	 45,
pages =	 "725-753",
year =	 2011,
class =	 "hJ",
journal =	 "International Journal of Software Engineering and
Knowledge Engineering"
}

@INPROCEEDINGS{czer11,
author =	 {Czerwonka, J. and Das, R. and Nagappan, N. and
Tarvo, A. and Teterev, A.},
booktitle =	 {Software Testing, Verification and Validation
(ICST), 2011 IEEE Fourth International Conference
on},
title =	 {CRANE: Failure Prediction, Change Analysis and Test
Prioritization in Practice -- Experiences from
Windows},
year =	 2011,
month =	 {march},
pages =	 {357 -366},
}
@ARTICLE{koc11b,
author =	 "E. Kocaguneli and T. Menzies and A. Bener and
J. Keung",
journal =	 "IEEE Transactions on Software Engineering",
title =	 "Exploiting the Essential Assumptions of
Analogy-Based Effort Estimation",
year =	 2012,
volume =	 28,
issue =	 2,
pages =	 "425-438",
note =	 "Available from
\url{http://menzies.us/pdf/11teak.pdf}",
class =	 "hJ"
}
@book{bird2015art,
title={The Art and Science of Analyzing Software Data},
author={Bird, Christian and Menzies, Tim and Zimmermann, Thomas},
year={2015},
publisher={Elsevier}
}
@article{me09b,
title =	 "On the Relative Value of Cross-Company and
Within-Company Data for Defect Prediction",
author =	 "B. Turhan and Tim Menzies and A. Bener and
J. Distefano",
year =	 2009,
journal =	 "Empirical Software Engineering",
volume =	 68,
number =	 2,
pages =	 "278-290",
note =	 "Available from
\url{http://menzies.us/pdf/08ccwc.pdf}",
class =	 "hJ"
}
@inproceedings{savor2016continuous,
title={Continuous deployment at Facebook and OANDA},
author={Savor, Tony and Douglas, Mitchell and Gentili, Michael and
Williams, Laurie and Beck, Kent and Stumm, Michael},
booktitle={Proceedings of the 38th International Conference on Software
Engineering Companion},
pages={21--30},
year={2016},
organization={ACM}
}
@inproceedings{linares2014mining,
title={Mining energy-greedy API usage patterns in Android apps: an
empirical study},
author={Linares-V{\'a}squez, Mario and Bavota, Gabriele and
Bernal-C{\'a}rdenas, Carlos and Oliveto, Rocco and Di Penta, Massimiliano
and Poshyvanyk, Denys},
booktitle={Proceedings of the 11th Working Conference on Mining Software
Repositories},
pages={2--11},
year={2014},
organization={ACM}
}
@inproceedings{theisen15,
year=2015,
title="Approximating Attack Surfaces with Stack Traces",
author="Christopher Theisen and  Kim Herzig and  Patrick Morrison and
Brendan Murphy and  Laurie Williams",
booktitle="ICSE'15"
}
@article{me13c,
title =	 {Software Analytics: So What?},
author =	 {Zimmermann, Thomas and Menzies, Tim},
journal =	 {IEEE Software},
volume =	 {30},
number =	 {4},
pages =	 {0031--37},
year =	 {2013},
}
@Inproceedings{export:208800,
abstract     = {<p>In this paper, we present the results from two surveys
related to data science
applied to software engineering. The first survey solicited questions that
software engineers would like data scientists to investigate about software,
about software processes and practices, and about software engineers. Our
analyses resulted in a list of 145 questions grouped into 12 categories. The
second survey asked a different pool of software engineers to rate these 145
questions and identify the most important ones to work on first. Respondents
favored questions that focus on how customers typically use their
applications.
We also saw opposition to questions that assess the performance of
individual
employees or compare them with one another. Our categorization and catalog
of 145
questions can help researchers, practitioners, and educators to more easily
focus
their efforts on topics that are important to the software industry.</p>

<p>The data appendix for this paper is here:
http://research.microsoft.com/apps/pubs/?id=200784.</p>},
author       = {Andrew Begel and Thomas Zimmermann},
booktitle    = {Proceedings of the 36th International Conference on
Software Engineering (ICSE
2014)},
month        = {June},
publisher    = {ACM},
title        = {Analyze This! 145 Questions for Data Scientists in Software
Engineering},
url          =
{http://research.microsoft.com/apps/pubs/default.aspx?id=208800},
year         = {2014},
}
@inproceedings{ostrand04,
author =	 {Ostrand, Thomas J. and Weyuker, Elaine J. and Bell,
Robert M.},
title =	 {Where the bugs are},
booktitle =	 {ISSTA '04: Proceedings of the 2004 ACM SIGSOFT
international symposium on Software testing and
analysis},
year =	 2004,
pages =	 {86--96},
publisher =	 {ACM},
address =	 {New York, NY, USA},
}
@article{me07e,
author =	 "Tim Menzies and Alex Dekhtyar and Justin Distefano
and Jeremy Greenwald",
title =	 "Problems with Precision",
note =	 "\url{http://menzies.us/pdf/07precision.pdf}",
journal =	 "IEEE Transactions on Software Engineering",
month =	 "September",
year =	 2007,
class =	 "hJ"
}

@article{me07b,
author =	 "Tim Menzies and Jeremy Greenwald and Art Frank",
year =	 2007,
month =	 "January",
title =	 "Data Mining Static Code Attributes to Learn Defect
Predictors",
journal =	 "IEEE Transactions on Software Engineering",
note =	 "Available from
\url{http://menzies.us/pdf/06learnPredict.pdf}",
class =	 "hJ"
}

@article{problems_with_pr,
author = {Menzies, Tim and Dekhtyar, Alex and Distefano, Justin and Greenwald, Jeremy},
title = {Problems with Precision: A Response to "Comments on 'Data Mining Static Code Attributes to Learn Defect Predictors'"},
journal = {IEEE Trans. Softw. Eng.},
issue_date = {September 2007},
volume = {33},
number = {9},
month = sep,
year = {2007},
issn = {0098-5589},
pages = {637--640},
numpages = {4},
url = {http://dx.doi.org/10.1109/TSE.2007.70721},
doi = {10.1109/TSE.2007.70721},
acmid = {1314090},
publisher = {IEEE Press},
address = {Piscataway, NJ, USA},
}
@inproceedings{panjer,
author = {Panjer, Lucas D.},
title = {Predicting Eclipse Bug Lifetimes},
booktitle = {Proceedings of the Fourth International Workshop on Mining Software
Repositories},
series = {MSR '07},
year = {2007},
isbn = {0-7695-2950-X},
pages = {29--},
url = {http://dx.doi.org/10.1109/MSR.2007.25},
doi = {10.1109/MSR.2007.25},
acmid = {1269043},
publisher = {IEEE Computer Society},
address = {Washington, DC, USA},
}
@inproceedings{kikas16,
author = {Kikas, Riivo and Dumas, Marlon and Pfahl, Dietmar},
title = {Using Dynamic and Contextual Features to Predict Issue Lifetime in GitHub
Projects},
booktitle = {Proceedings of the 13th International Conference on Mining Software
Repositories},
series = {MSR '16},
year = {2016},
isbn = {978-1-4503-4186-8},
location = {Austin, Texas},
pages = {291--302},
numpages = {12},
url = {http://doi.acm.org/10.1145/2901739.2901751},
doi = {10.1145/2901739.2901751},
acmid = {2901751},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {issue lifetime prediction, issue tracking, mining software repositories},
}

@inproceedings{tempero,
author = {Tempero, Ewan and Anslow, Craig and Dietrich, Jens and Han, Ted and Li, Jing
and Lumpe, Markus and Melton, Hayden and Noble, James},
title = {Qualitas Corpus: A Curated Collection of Java Code for Empirical Studies},
booktitle = {2010 Asia Pacific Software Engineering Conference (APSEC2010)},
pages = {336--345},
month = dec,
year = {2010},
doi = {http://dx.doi.org/10.1109/APSEC.2010.46}
}
@article{rees,
author = {Rees-jones, Mitch and Martin, Matthew and College, Colby and Menzies, Tim},
file = {:Users/rkrsn/Documents/Mendeley Desktop/Rees-jones et al. - Unknown - Better
Predictors for Issue Lifetime.pdf:pdf},
pages = {1--8},
year = {2017},
journal = {{}},
title = {{Better Predictors for Issue Lifetime}}
}

@inproceedings{zhang13issue,
author = {Zhang, Hongyu and Gong, Liang and Versteeg, Steve},
title = {Predicting Bug-fixing Time: An Empirical Study of Commercial Software
Projects},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
series = {ICSE '13},
year = {2013},
isbn = {978-1-4673-3076-3},
location = {San Francisco, CA, USA},
pages = {1042--1051},
numpages = {10},
url = {http://dl.acm.org/citation.cfm?id=2486788.2486931},
acmid = {2486931},
publisher = {IEEE Press},
address = {Piscataway, NJ, USA},
}

@inproceedings{giger,
author = {Giger, Emanuel and Pinzger, Martin and Gall, Harald},
title = {Predicting the Fix Time of Bugs},
booktitle = {Proceedings of the 2Nd International Workshop on Recommendation Systems
for Software Engineering},
series = {RSSE '10},
year = {2010},
isbn = {978-1-60558-974-9},
location = {Cape Town, South Africa},
pages = {52--56},
numpages = {5},
url = {http://doi.acm.org/10.1145/1808920.1808933},
doi = {10.1145/1808920.1808933},
acmid = {1808933},
publisher = {ACM},
address = {New York, NY, USA},
}
@article{font16,
abstract = {Several code smell detection tools have been developed providing different
results, because smells can be subjectively interpreted, and hence detected, in
different ways. In this paper, we perform the largest experiment of applying machine
learning algorithms to code smells to the best of our knowledge. We experiment 16
different machine-learning algorithms on four code smells (Data Class, Large Class,
Feature Envy, Long Method) and 74 software systems, with 1986 manually validated code
smell samples. We found that all algorithms achieved high performances in the
cross-validation data set, yet the highest performances were obtained by J48 and Random
Forest, while the worst performance were achieved by support vector machines. However,
the lower prevalence of code smells, i.e., imbalanced data, in the entire data set
caused varying performances that need to be addressed in the future studies. We
conclude that the application of machine learning to the detection of these code smells
can provide high accuracy ({\textgreater}96 {\%}), and only a hundred training examples
are needed to reach at least 95 {\%} accuracy.},
author = {{Arcelli Fontana}, Francesca and M{\"{a}}ntyl{\"{a}}, Mika V. and Zanoni,
Marco and Marino, Alessandro},
doi = {10.1007/s10664-015-9378-4},
file = {:Users/rkrsn/Documents/Mendeley Desktop/Arcelli Fontana et al. - 2016 -
Comparing and experimenting machine learning techniques for code smell
detection(2).pdf:pdf},
issn = {1382-3256},
journal = {Empir. Softw. Eng.},
keywords = {Benchmark for code smell detection,Code smells detection,Machine learning
techniques},
month = {jun},
number = {3},
pages = {1143--1191},
publisher = {Empirical Software Engineering},
title = {{Comparing and experimenting machine learning techniques for code smell
detection}},
url = {http://dx.doi.org/10.1007/s10664-015-9378-4
http://link.springer.com/10.1007/s10664-015-9378-4},
volume = {21},
year = {2016}
}
@INPROCEEDINGS{maiga12b,
author={A. Maiga and N. Ali and N. Bhattacharya and A. Sabané and Y. G. Guéhéneuc and
E. Aimeur},
booktitle={2012 19th Working Conference on Reverse Engineering},
title={SMURF: A SVM-based Incremental Anti-pattern Detection Approach},
year={2012},
pages={466-475},
keywords={learning (artificial intelligence);program diagnostics;software development
management;software maintenance;support vector machines;BDTEX;DETEX;SMURF;SVM-based
incremental antipattern detection approach;development activities;intersystem
configurations;intrasystem configurations;machine learning technique;maintenance
activities;software development projects;source code;support vector
machines;Accuracy;Kernel;Maintenance engineering;Measurement;Support vector
machines;Training;Anti-pattern;empirical software engineering;program
comprehension;program maintenance},
doi={10.1109/WCRE.2012.56},
ISSN={1095-1350},
month={Oct},}
@INPROCEEDINGS{maiga12,
author={J. Yang and K. Hotta and Y. Higo and H. Igaki and S. Kusumoto},
booktitle={2012 6th International Workshop on Software Clones (IWSC)},
title={Filtering clones for individual user based on machine learning analysis},
year={2012},
pages={76-77},
keywords={human computer interaction;learning (artificial intelligence);software
engineering;clones filtering;code clones detetector;machine learning analysis;software
source code;Cloning;Detectors;Educational institutions;Electronic mail;Information
science;Machine learning;Machine learning algorithms;classify;code clone
detector;filtering;judgment of user;machine learning;token-based},
doi={10.1109/IWSC.2012.6227872},
month={June},}
@inproceedings{Maiga:2012:SVM:2351676.2351723,
author = {Maiga, Abdou and Ali, Nasir and Bhattacharya, Neelesh and Saban{\'e}, Aminata
and Gu{\'e}h{\'e}neuc, Yann-Ga\"{e}l and Antoniol, Giuliano and A\"{\i}meur, Esma},
title = {Support Vector Machines for Anti-pattern Detection},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated
Software Engineering},
series = {ASE 2012},
year = {2012},
isbn = {978-1-4503-1204-2},
location = {Essen, Germany},
pages = {278--281},
numpages = {4},
url = {http://doi.acm.org/10.1145/2351676.2351723},
doi = {10.1145/2351676.2351723},
acmid = {2351723},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {Anti-pattern, empirical software engineering, program comprehension,
program maintenance},
}
@article{Khomh11,
title = "BDTEX: A GQM-based Bayesian approach for the detection of antipatterns ",
journal = "Journal of Systems and Software ",
volume = "84",
number = "4",
pages = "559 - 572",
year = "2011",
note = "The Ninth International Conference on Quality Software ",
issn = "0164-1212",
doi = "http://dx.doi.org/10.1016/j.jss.2010.11.921",
url = "//www.sciencedirect.com/science/article/pii/S0164121210003225",
author = "Foutse Khomh and Stephane Vaucher and Yann-Gaël Guéhéneuc and Houari
Sahraoui",
keywords = "Code smells",
keywords = "Antipatterns",
keywords = "Detection "
}


@INPROCEEDINGS{khomh09,
author={F. Khomh and S. Vaucher and Y. G. Guéhéneuc and H. Sahraoui},
booktitle={2009 Ninth International Conference on Quality Software},
title={A Bayesian Approach for the Detection of Code and Design Smells},
year={2009},
pages={305-314},
keywords={belief networks;software quality;uncertainty handling;Bayesian approach;Blob
antipattern;GanttProject v1.10.2;Xerces v2.7.0;code smell detection;context specific
detection;design smell detection;detection rules;machine learning technique;open source
program;probabilistic model;program quality;quality analysts effort;uncertainty
handling;Bayesian methods;Context modeling;Electronic mail;Impedance;Machine
learning;Open source software;Quality assessment;Software quality;Software
systems;Uncertainty;bayesian belief networks;code smells;design smells;software
quality},
doi={10.1109/QSIC.2009.47},
ISSN={1550-6002},
month={Aug},}
@article{kreimer05,
abstract = {Criteria for software quality measurement depend on the application area. In large software systems criteria like maintainability, comprehensibility and extensibility play an important role. My aim is to identify design flaws in software systems automatically and thus to avoid “bad” — incomprehensible, hardly expandable and changeable — program structures. Depending on the perception and experience of the searching engineer, design flaws are interpreted in a different way. I propose to combine known methods for finding design flaws on the basis of metrics with machine learning mechanisms, such that design flaw detection is adaptable to different views. This paper presents the underlying method, describes an analysis tool for Java programs and shows results of an initial case study. },
annote = {Proceedings of the Fifth Workshop on Language Descriptions, Tools, and Applications (LDTA 2005)Language Descriptions, Tools, and Applications 2005},
author = {Kreimer, Jochen},
doi = {http://dx.doi.org/10.1016/j.entcs.2005.02.059},
issn = {1571-0661},
journal = {Electronic Notes in Theoretical Computer Science},
keywords = {Design flaw,code smell,machine learning,object-oriented design,program analysis,refactoring,software quality},
number = {4},
pages = {117--136},
title = {{Adaptive Detection of Design Flaws}},
url = {//www.sciencedirect.com/science/article/pii/S1571066105051844},
volume = {141},
year = {2005}
}
@article{kocaguneli2012,
author = {Kocaguneli, Ekrem and Menzies, Tim and Mendes, Emilia},
doi = {10.1007/s10664-014-9300-5},
file = {:home/rkrsn/Documents/Mendeley Desktop/Kocaguneli, Menzies, Mendes - 2015 - Transfer learning in effort estimation.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
month = {jun},
number = {3},
pages = {813--843},
publisher = {Springer US},
title = {{Transfer learning in effort estimation}},
url = {http://link.springer.com/10.1007/s10664-014-9300-5},
volume = {20},
year = {2015}
}
@article{kitchen,
author = {Kitchenham, Barbara A. and Mendes, Emilia and Travassos, Guilherme H.},
title = {Cross Versus Within-Company Cost Estimation Studies: A Systematic Review},
journal = {IEEE Trans. Softw. Eng.},
issue_date = {May 2007},
volume = {33},
number = {5},
month = may,
year = {2007},
issn = {0098-5589},
pages = {316--329},
numpages = {14},
url = {http://dx.doi.org/10.1109/TSE.2007.1001},
doi = {10.1109/TSE.2007.1001},
acmid = {1263528},
publisher = {IEEE Press},
address = {Piscataway, NJ, USA},
keywords = {Cost estimation, Cost estimation, management, systematic review, software engineering., management, software engineering., systematic review},
}
@ARTICLE{hall2012,
author={T. Hall and S. Beecham and D. Bowes and D. Gray and S. Counsell},
journal={IEEE Transactions on Software Engineering},
title={A Systematic Literature Review on Fault Prediction Performance in Software Engineering},
year={2012},
volume={38},
number={6},
pages={1276-1304},
keywords={Bayes methods;regression analysis;software fault tolerance;software quality;contextual information;cost reduction;fault prediction models;fault prediction performance;fault prediction study;feature selection;independent variables;logistic regression;methodological information;naive Bayes;predictive performance;reliable methodology;simple modeling techniques;software engineering;software quality;systematic literature review;Analytical models;Context modeling;Data models;Fault diagnosis;Predictive models;Software testing;Systematics;Systematic literature review;software fault prediction},
doi={10.1109/TSE.2011.103},
ISSN={0098-5589},
month={Nov},}
@article{vcboost16,
abstract = {It is well-known that software defect prediction is one of the
most important tasks for software quality improvement. The use of defect
predictors allows test engineers to focus on defective modules. Thereby
testing resources can be allocated effectively and the quality assurance
costs can be reduced. For within-project defect prediction (WPDP), there
should be sufficient data within a company to train any prediction model.
Without such local data, cross-project defect prediction (CPDP) is feasible
since it uses data collected from similar projects in other companies.
Software defect datasets have the class imbalance problem increasing the
difficulty for the learner to predict defects. In addition, the impact of
imbalanced data on the real performance of models can be hidden by the
performance measures chosen. We investigate if the class imbalance learning
can be beneficial for CPDP. In our approach, the asymmetric
misclassification cost and the similarity weights obtained from
distributional characteristics are closely associated to guide the
appropriate resampling mechanism. We performed the effect size A-statistics
test to evaluate the magnitude of the improvement. For the statistical
significant test, we used Wilcoxon rank-sum test. The experimental results
show that our approach can provide higher prediction performance than both
the existing CPDP technique and the existing class imbalance technique.},
author = {Ryu, Duksan and Choi, Okjoo and Baik, Jongmoon},
doi = {10.1007/s10664-014-9346-4},
file = {:Users/rkrsn/Documents/Mendeley Desktop/Ryu, Choi, Baik - 2016 -
Value-cognitive boosting with a support vector machine for cross-project
defect prediction.pdf:pdf},
issn = {1382-3256},
journal = {Empir. Softw. Eng.},
keywords = {Boosting,Class imbalance,Cross-project defect
prediction,Transfer learning},
mendeley-groups = {Bellwether Journal},
month = {feb},
number = {1},
pages = {43--71},
title = {{Value-cognitive boosting with a support vector machine for
cross-project defect prediction}},
url = {http://link.springer.com/10.1007/s10664-014-9346-4},
volume = {21},
year = {2016}
}
@book{fowler99,
title = {Refactoring: Improving the Design of Existing Code},
year = {1999},
author ={M. Fowler and K. Beck and J.  Brant and W. Opdyke and D Roberts},
publisher = {Addison-Wesley Longman},
address = {Boston, MA, USA},
}
@inproceedings{chen2005,
title={Feature subset selection can improve software cost estimation accuracy},
author={Chen, Zhihao and Menzies, Tim and Port, Dan and Boehm, Barry},
booktitle={ACM SIGSOFT Software Engineering Notes},
volume={30},
pages={1--6},
year={2005},
organization={ACM}
}
@inproceedings{jing15,
title="Heterogeneous cross-company defect prediction by unified metric representation and CCA-based transfer learning",
author=" X. Jing and G. Wu and X. Dong and F. Qi and B. Xu",
year =2015,
booktitle="FSE'15"
}
@article{yamashita2013code,
title={Code smells as system-level indicators of maintainability: An empirical study},
author={Yamashita, Aiko and Counsell, Steve},
journal={Journal of Systems and Software},
volume={86},
number={10},
pages={2639--2653},
year={2013},
publisher={Elsevier}
}
@inproceedings{olbrich2010all,
title={Are all code smells harmful? A study of God Classes and Brain Classes in the evolution of three open source systems},
author={Olbrich, Steffen M and Cruzes, Daniela S and Sj{\o}berg, Dag IK},
booktitle={Software Maintenance (ICSM), 2010 IEEE International Conference on},
pages={1--10},
year={2010}
}
@article{krishna2016b,
author    = {Rahul Krishna and
Tim Menzies and
Lucas Layman},
title     = {{Less is More: Minimizing Code Reorganization using XTREE}},
journal   = {CoRR},
volume    = {abs/1609.03614},
year      = {2016},
url       = {http://arxiv.org/abs/1609.03614},
timestamp = {Mon, 03 Oct 2016 17:51:10 +0200},
biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/KrishnaML16},
bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{font12,
title={Automatic detection of bad smells in code: An experimental assessment.},
author={Fontana, Francesca Arcelli and Braione, Pietro and Zanoni, Marco},
journal={Journal of Object Technology},
volume={11},
number={2},
pages={5--1},
year={2012}
}
@article{unstab,
abstract = {The goal of science is conclusion stability, i.e. to discover some effect X that holds in multiple situations. Sadly, there are all too few examples of stable conclusions in software engineering (SE). In fact, the typical result is conclusion instability where what is true for project one, does not hold for project two. We can find numerous studies of the following form: there is as much evidence for as against the argument that some aspect X adds value to a software project. Below are four examples of this type of problem which we believe to be endemic within SE.},
author = {Menzies, Tim and Shepperd, Martin},
doi = {10.1007/s10664-011-9193-5},
file = {:Users/rkrsn/Documents/Mendeley Desktop/Menzies, Shepperd - 2012 - Special issue on repeatable results in software engineering prediction(2).pdf:pdf},
issn = {1382-3256},
journal = {Empir. Softw. Eng.},
month = {feb},
number = {1-2},
pages = {1--17},
title = {{Special issue on repeatable results in software engineering prediction}},
url = {http://link.springer.com/10.1007/s10664-011-9193-5},
volume = {17},
year = {2012}
}
@inproceedings{krishna16a,
title={{Too Much Automation? The Bellwether Effect and Its Implications for Transfer Learning}},
author={Krishna, Rahul and Menzies, Tim and Fu, Wei},
year=2016,
booktitle="ASE'16"
}
@inproceedings{yama13,
title={Exploring the impact of inter-smell relations on software maintainability: An empirical study},
author={Yamashita, Aiko and Moonen, Leon},
booktitle={Proceedings of the 2013 International Conference on Software Engineering},
pages={682--691},
year={2013},
organization={IEEE Press}
}
@inproceedings{zazworka2011investigating,
title={Investigating the impact of design debt on software quality},
author={Zazworka, Nico and Shaw, Michele A and Shull, Forrest and Seaman, Carolyn},
booktitle={Proceedings of the 2nd Workshop on Managing Technical Debt},
pages={17--23},
year={2011},
organization={ACM}
}
@article{Kocaguneli2014,
abstract = {When projects lack sufficient local data to make predictions, they try to transfer information from other projects. How can we best support this process? In the field of software engineering, transfer learning has been shown to be effective for defect prediction. This paper checks whether it is possible to build transfer learners for software effort estimation. We use data on 154 projects from 2 sources to investigate transfer learning between different time intervals and 195 projects from 51 sources to provide evidence on the value of transfer learning for traditional cross-company learning problems. We find that the same transfer learning method can be useful for transfer effort estimation results for the cross-company learning problem and the cross-time learning problem. It is misguided to think that: (1) Old data of an organization is irrelevant to current context or (2) data of another organization cannot be used for local solutions. Transfer learning is a promising research direction that transfers relevant cross data between time intervals and domains.},
author = {Kocaguneli, Ekrem and Menzies, Tim and Mendes, Emilia},
doi = {10.1007/s10664-014-9300-5},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Data mining,Effort estimation,Transfer learning,k-NN},
number = {3},
pages = {813--843},
title = {{Transfer learning in effort estimation}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84929522024{\&}partnerID=tZOtx3y1},
volume = {20},
year = {2014}
}

@inproceedings{nagappan05,
title =	"Static Analysis Tools as Early Indicators of
Pre-Release Defect Density",
author =	"Nachiappan Nagappan and Thomas Ball",
year =	2005,
booktitle =	"ICSE 2005, St. Louis"
}
@inproceedings{me02f,
title =	"Model-based Tests of Truisms",
author =	"Tim Menzies and David Raffo and Siri-on Setamanit
and Ying Hu and Sina Tootoonian",
booktitle =	"Proceedings of IEEE ASE 2002",
year =	2002,
note =	"Available from
\url{http://menzies.us/pdf/02truisms.pdf}",
class =	"hC"
}
@inproceedings{lewis13,
author = {Lewis, Chris and Lin, Zhongpeng and Sadowski, Caitlin and Zhu, Xiaoyan and Ou, Rong and Whitehead Jr., E. James},
title = {Does Bug Prediction Support Human Developers? Findings from a Google Case Study},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
series = {ICSE '13},
year = {2013},
isbn = {978-1-4673-3076-3},
location = {San Francisco, CA, USA},
pages = {372--381},
numpages = {10},
url = {http://dl.acm.org/citation.cfm?id=2486788.2486838},
acmid = {2486838},
publisher = {IEEE Press},
address = {Piscataway, NJ, USA},
}
@INPROCEEDINGS{yang12,
author={J. Yang and K. Hotta and Y. Higo and H. Igaki and S. Kusumoto},
booktitle={2012 6th International Workshop on Software Clones (IWSC)},
title={Filtering clones for individual user based on machine learning analysis},
year={2012},
pages={76-77},
keywords={human computer interaction;learning (artificial intelligence);software engineering;clones filtering;code clones detetector;machine learning analysis;software source code;Cloning;Detectors;Educational institutions;Electronic mail;Information science;Machine learning;Machine learning algorithms;classify;code clone detector;filtering;judgment of user;machine learning;token-based},
doi={10.1109/IWSC.2012.6227872},
month={June},}
@inproceedings{tosun09,
author =	"A. Tosun and A. Bener and B. Turhan",
title =	"Practical Considerations of Deploying AI in Defect
Prediction: A Case Study within the {T}urkish
Telecommunication Industry",
year =	2009,
booktitle =	"PROMISE'09"
}
@inproceedings{tosun10,
year =	2010,
author =	"A. Tosun and A. Bener and R. Kale",
title =	"{AI}-Based Software Defect Predictors: Applications
and Benefits in a Case Study",
booktitle =	"Twenty-Second IAAI Conference on Artificial
Intelligence"
}
@inProceedings{shu02,
author =	"F. Shull and V.R. Basili ad B. Boehm and A.W. Brown
and P. Costa and M. Lindvall and D. Port and I. Rus
and R. Tesoriero and M.V. Zelkowitz",
title =	"What We Have Learned About Fighting Defects",
booktitle =	"Proceedings of 8th International Software Metrics
Symposium, Ottawa, Canada",
year =	2002,
pages =	"249-258",
}
@article{fagan76,
author =	"M. Fagan",
title =	"Design and Code Inspections to Reduce Errors in
Program Development",
journal =	"IBM Systems Journal",
volume =	15,
number =	3,
year =	1976,
page =	"182-211"
}

@inproceedings{Nam2013,
abstract = {Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance. © 2013 IEEE.},
author = {Nam, Jaechang and Pan, Sinno Jialin and Kim, Sunghun},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1109/ICSE.2013.6606584},
isbn = {9781467330763},
issn = {02705257},
keywords = {cross-project defect prediction,empirical software engineering,transfer learning},
pages = {382--391},
title = {{Transfer defect learning}},
year = {2013}
}


@article{turhan2009relative,
title={On the relative value of cross-company and within-company data for defect prediction},
author={Turhan, Burak and Menzies, Tim and Bener, Ay{\c{s}}e B and Di Stefano, Justin},
journal={Empirical Software Engineering},
volume={14},
number={5},
pages={540--578},
year={2009},
publisher={Springer}
}

@inproceedings{he2013learning,
title={Learning from open-source projects: An empirical study on defect prediction},
author={He, Zhimin and Peters, Fayola and Menzies, Tim and Yang, Ye},
booktitle={Empirical Software Engineering and Measurement, 2013 ACM/IEEE International Symposium on},
pages={45--54},
year={2013},
organization={IEEE}
}

@article{Hall2011,
abstract = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs and improve the quality of software. Objective: We investigate how the context of models, the independent variables used and the modelling techniques applied, influence the performance of fault prediction models. Method:We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesise the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modelling techniques such as Na{\"{\i}}ve Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology and performance comprehensively.},
author = {Hall, Tracy and Beecham, Sarah and Bowes, David and Gray, David and Counsell, Steve},
doi = {10.1109/TSE.2011.103},
isbn = {9781612081656},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
number = {6},
pages = {1276--1304},
title = {{A Systematic Review of Fault Prediction Performance in Software Engineering}},
volume = {38},
year = {2011}
}



@book{rakitin01,
author =	"S.R. Rakitin",
title =	"Software Verification and Validation for
Practitioners and Managers, Second Edition",
publisher =	"Artech House",
isbn =	"1-58053-296-9",
year =	2001
}
@InProceedings{LowryBK98,
title={Towards a theory for integration of mathematical verification and empirical testing},
author={Lowry, Michael and Boyd, Mark and Kulkami, Deepak},
booktitle={Automated Software Engineering, 1998. Proceedings. 13th IEEE International Conference on},
pages={322--331},
year={1998},
organization={IEEE}
}


@inproceedings{rahman14:icse,
title={Comparing static bug finders and statistical prediction},
author={Rahman, Foyzur and Khatri, Sameer and Barr, Earl T and Devanbu, Premkumar},
booktitle={Proc. International Conference on Software Engineering},
pages={424--434},
year={2014},
organization={ACM}
}

@inproceedings{kocaguneli2011find,
title={How to find relevant data for effort estimation?},
author={Kocaguneli, Ekrem and Menzies, Tim},
booktitle={Empirical Software Engineering and Measurement (ESEM), 2011 International Symposium on},
pages={255--264},
year={2011},
organization={IEEE}
}

@inproceedings{peters15,
abstract = {Before a community can learn general principles, it must share individual experiences. Data sharing is the fundamental step of cross project defect prediction, i.e. the process of using data from one project to predict for defects in another. Prior work on secure data sharing allowed data owners to share their data on a single-party basis for defect prediction via data minimization and obfuscation. However the studied method did not consider that bigger data required the data owner to share more of their data. In this paper, we extend previous work with LACE2 which reduces the amount of data shared by using multi-party data sharing. Here data owners incrementally add data to a cache passed among them and contribute "interesting" data that are not similar to the current content of the cache. Also, before data owner i passes the cache to data owner j, privacy is preserved by applying obfuscation algorithms to hide project details. The experiments of this paper show that (a) LACE2 is comparatively less expensive than the single-party approach and (b) the multi-party approach of LACE2 yields higher privacy than the prior approach without damaging predictive efficacy (indeed, in some cases, LACE2 leads to better defect predictors).},
author = {Peters, Fayola and Menzies, Tim and Layman, Lucas},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1109/ICSE.2015.92},
isbn = {9781479919345},
issn = {02705257},
pages = {801--811},
title = {{LACE2: Better privacy-preserving data sharing for cross project defect prediction}},
volume = {1},
year = {2015}
}

@inproceedings{dekhtyar2004,
title={Text is software too},
author={Dekhtyar, Alexander and Hayes, Jane Huffman and Menzies, Tim},
booktitle={MSR 2004: International Workshop on Mining Software Repositories at ICSE’04: Edinburgh, Scotland},
pages={22},
year={2004}
}
@inproceedings{bell2006,
title={Looking for bugs in all the right places},
author={Bell, Robert M and Ostrand, Thomas J and Weyuker, Elaine J},
booktitle={Proceedings of the 2006 international symposium on Software testing and analysis},
pages={61--72},
year={2006},
organization={ACM}
}
@inproceedings{arisholm2006,
title={Predicting fault-prone components in a java legacy system},
author={Arisholm, Erik and Briand, Lionel C},
booktitle={Proceedings of the 2006 ACM/IEEE international symposium on Empirical software engineering},
pages={8--17},
year={2006},
organization={ACM}
}
@article{Chen2015,
abstract = {Context: Software defect prediction has been widely studied
based on various machine-learning algorithms. Previous studies usually
focus on within-company defects prediction (WCDP), but lack of training
data in the early stages of software testing limits the efficiency of WCDP
in practice. Thus, recent research has largely examined the cross-company
defects prediction (CCDP) as an alternative solution. Objective: However,
the gap of different distributions between cross-company (CC) data and
withincompany (WC) data usually makes it difficult to build a high-quality
CCDP model. In this paper, a novel algorithm named Double Transfer
Boosting (DTB) is introduced to narrow this gap and improve the
performance of CCDP by reducing negative samples in CC data. Method: The
proposed DTB model integrates two levels of data transfer: first, the data
gravitation method reshapes the whole distribution of CC data to fit WC
data. Second, the transfer boosting method employs a small ratio of
labeled WC data to eliminate negative instances in CC data. Results: The
empirical evaluation was conducted based on 15 publicly available
datasets. CCDP experiment results indicated that the proposed model
achieved better overall performance than compared CCDP models. DTB was
also compared to WCDP in two different situations. Statistical analysis
suggested that DTB performed significantly better than WCDP models trained
by limited samples and produced comparable results to WCDP with sufficient
training data. Conclusions: DTB reforms the distribution of CC data from
different levels to improve the performance of CCDP, and experimental
results and analysis demonstrate that it could be an effective model for
early software defects detection.},
author = {Chen, Lin and Fang, Bin and Shang, Zhaowei and Tang, Yuanyan},
doi = {10.1016/j.infsof.2015.01.014},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Cross-company defects prediction,Software fault
prediction,Transfer learning},
number = {1},
pages = {67--77},
title = {{Negative samples reduction in cross-company software defects
prediction}},
volume = {62},
year = {2015}
}
@article{Ma2012,
abstract = {Context: Software defect prediction studies usually built
models using within-company data, but very few focused on the prediction
models trained with cross-company data. It is difficult to employ these
models which are built on the within-company data in practice, because of
the lack of these local data repositories. Recently, transfer learning has
attracted more and more attention for building classifier in target domain
using the data from related source domain. It is very useful in cases when
distributions of training and test instances differ, but is it appropriate
for cross-company software defect prediction? Objective: In this paper, we
consider the cross-company defect prediction scenario where source and
target data are drawn from different companies. In order to harness cross
company data, we try to exploit the transfer learning method to build
faster and highly effective prediction model. Method: Unlike the prior
works selecting training data which are similar from the test data, we
proposed a novel algorithm called Transfer Naive Bayes (TNB), by using the
information of all the proper features in training data. Our solution
estimates the distribution of the test data, and transfers cross-company
data information into the weights of the training data. On these weighted
data, the defect prediction model is built. Results: This article presents
a theoretical analysis for the comparative methods, and shows the
experiment results on the data sets from different organizations. It
indicates that TNB is more accurate in terms of AUC (The area under the
receiver operating characteristic curve), within less runtime than the
state of the art methods. Conclusion: It is concluded that when there are
too few local training data to train good classifiers, the useful
knowledge from different-distribution training data on feature level may
help. We are optimistic that our transfer learning method can guide
optimal resource allocation strategies, which may reduce software testing
cost and increase effectiveness of software testing process.
{\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Ma, Ying and Luo, Guangchun and Zeng, Xue and Chen, Aiguo},
doi = {10.1016/j.infsof.2011.09.007},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Different distribution,Machine learning,Naive Bayes,Software
defect prediction,Transfer learning},
number = {3},
pages = {248--256},
title = {{Transfer learning for cross-company software defect prediction}},
volume = {54},
year = {2012}
}
@article{zhang15,
year={2015},
issn={1382-3256},
journal={Empirical Software Engineering},
doi={10.1007/s10664-015-9396-2},
title={Towards building a universal defect prediction model with rank transformed predictors},
url={http://dx.doi.org/10.1007/s10664-015-9396-2},
publisher={Springer US},
keywords={Universal defect prediction model; Defect prediction; Context factors; Rank transformation; Large-scale; Software quality},
author={Zhang, Feng and Mockus, Audris and Keivanloo, Iman and Zou, Ying},
pages={1-39},
language={English}
}
@inproceedings{zhang2014,
title={Towards building a universal defect prediction model},
author={Zhang, Feng and Mockus, Audris and Keivanloo, Iman and Zou, Ying},
booktitle={Proceedings of the 11th Working Conference on Mining Software Repositories},
pages={182--191},
year={2014},
organization={ACM}
}
@article{Jing2015,
author = {Jing, Xiaoyuan and Wu, Fei and Dong, Xiwei and Qi, Fumin and Xu,
Baowen},
doi = {10.1145/2786805.2786813},
file = {:C$\backslash$:/Users/Rahul Krishna/Documents/Mendeley
Desktop/Jing et al. - 2015 - Heterogeneous Cross-Company Defect Prediction
by Unified Metric Representation and CCA-Based Transfer Learning
Cate.pdf:pdf},
isbn = {9781450336758},
journal = {Proceeding of the 10th Joint Meeting of the European Software
Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of
Software Engineering (ESEC/FSE 2015)},
keywords = {applies the model to,ccdp,cross-company defect prediction,data
from one or,learns a prediction,model by using training,multiple projects
of a,source company and then,the target company},
pages = {496--507},
title = {{Heterogeneous Cross-Company Defect Prediction by Unified Metric
Representation and CCA-Based Transfer Learning Categories and Subject
Descriptors}},
year = {2015}
}
@article{fu16,
title = "Tuning for software analytics: Is it really necessary? ",
journal = "Information and Software Technology ",
volume = "76",
number = "",
pages = "135 - 146",
year = "2016",
note = "",
issn = "0950-5849",
doi = "http://dx.doi.org/10.1016/j.infsof.2016.04.017",
url = "http://www.sciencedirect.com/science/article/pii/S0950584916300738",
author = "Wei Fu and Tim Menzies and Xipeng Shen",
keywords = "Defect prediction",
keywords = "CART",
keywords = "Random forest",
keywords = "Differential evolution",
keywords = "Search-based software engineering ",
abstract = "Abstract Context: Data miners have been widely used in software engineering to, say, generate defect predictors from static code measures. Such static code defect predictors perform well compared to manual methods, and they are easy to use and useful to use. But one of the “black arts” of data mining is setting the tunings that control the miner. Objective: We seek simple, automatic, and very effective method for finding those tunings. Method: For each experiment with different data sets (from open source \{JAVA\} systems), we ran differential evolution as an optimizer to explore the tuning space (as a first step) then tested the tunings using hold-out data. Results: Contrary to our prior expectations, we found these tunings were remarkably simple: it only required tens, not thousands, of attempts to obtain very good results. For example, when learning software defect predictors, this method can quickly find tunings that alter detection precision from 0% to 60%. Conclusion: Since (1) the improvements are so large, and (2) the tuning is so simple, we need to change standard methods in software analytics. At least for defect prediction, it is no longer enough to just run a data miner and present the result without conducting a tuning optimization study. The implication for other kinds of analytics is now an open and pressing issue. "
}


@article{Menzies2007a,
abstract = {Zhang and Zhang argue that predictors are useless unless they have high precison{\&}amp;recall. We have a different view, for two reasons. First, for SE data sets with large neg/pos ratios, it is often required to lower precision to achieve higher recall. Second, there are many domains where low precision detectors are useful.},
author = {Menzies, Tim and Dekhtyar, Alex and Distefano, Justin and Greenwald, Jeremy},
doi = {10.1109/TSE.2007.70721},
file = {:C$\backslash$:/Users/Rahul Krishna/Documents/Mendeley Desktop/Menzies et al. - 2007 - Problems with precision A response to Comments on 'data mining static code attributes to learn defect predictors.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Accuracy measures,Defect prediction,Empirical,Static code attributes},
month = {sep},
number = {9},
pages = {637--640},
title = {{Problems with Precision: A Response to "Comments on 'Data Mining Static Code Attributes to Learn Defect Predictors'"}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4288197},
volume = {33},
year = {2007}
}
@inproceedings{Peters2013,
abstract = {How can we find data for quality prediction? Early in the life
cycle, projects may lack the data needed to build such predictors. Prior
work assumed that relevant training data was found nearest to the local
project. But is this the best approach? This paper introduces the Peters
filter which is based on the following conjecture: When local data is
scarce, more information exists in other projects. Accordingly, this
filter selects training data via the structure of other projects. To
assess the performance of the Peters filter, we compare it with two other
approaches for quality prediction. Within-company learning and
cross-company learning with the Burak filter (the state-of-the-art
relevancy filter). This paper finds that: 1) within-company predictors are
weak for small data-sets; 2) the Peters filter+cross-company builds better
predictors than both within-company and the Burak filter+cross-company;
and 3) the Peters filter builds 64{\%} more useful predictors than both
within-company and the Burak filter+cross-company approaches. Hence, we
recommend the Peters filter for cross-company learning. {\textcopyright}
2013 IEEE.},
author = {Peters, Fayola and Menzies, Tim and Marcus, Andrian},
booktitle = {2013 10th Working Conference on Mining Software Repositories
(MSR)},
doi = {10.1109/MSR.2013.6624057},
isbn = {978-1-4673-2936-1},
issn = {21601852},
keywords = {Cross company,Data mining,Defect prediction},
month = {may},
pages = {409--418},
publisher = {IEEE},
title = {{Better cross company defect prediction}},
url =
{http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6624057},
year = {2013}
}

@inproceedings{zimm09,
title={Cross-project defect prediction: a large scale experiment on data vs. domain vs. process},
author={Zimmermann, Thomas and Nagappan, Nachiappan and Gall, Harald and Giger, Emanuel and Murphy, Brendan},
booktitle={Proceedings of the the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering},
pages={91--100},
year={2009},
organization={ACM}
}
@inproceedings{turhan11,
title={Empirical evaluation of mixed-project defect prediction models},
author={Turhan, Burak and Tosun, Ay{\c{s}}e and Bener, Ay{\c{s}}e},
booktitle={Software Engineering and Advanced Applications (SEAA), 2011 37th EUROMICRO Conference on},
pages={396--403},
year={2011},
organization={IEEE}
}
@book{rup12,
title={Simultaneous statistical inference},
author={Rupert Jr, G and others},
year={2012},
publisher={Springer Science \& Business Media}
}
@article {ben10,
author = {Benjamini, Yoav},
title = {Simultaneous and selective inference: Current successes and future challenges},
journal = {Biometrical Journal},
volume = {52},
number = {6},
publisher = {WILEY-VCH Verlag},
issn = {1521-4036},
url = {http://dx.doi.org/10.1002/bimj.200900299},
doi = {10.1002/bimj.200900299},
pages = {708--721},
keywords = {Aggregated safety analysis, False discovery rates, Familywise error rate, Functional magnetic resonance imaging, Multiple comparisons procedures},
year = {2010},
}

@article{zhang2008distribution,
title={On the distribution of software faults},
author={Zhang, Hongyu},
journal={IEEE Transactions on Software Engineering},
volume={34},
number={2},
pages={301},
year={2008},
publisher={IEEE Computer Society}
}
@article{koru2007identifying,
title={Identifying and characterizing change-prone classes in two large-scale open-source products},
author={Koru, A G{\"u}ne{\c{s}} and Liu, Hongfang},
journal={Journal of Systems and Software},
volume={80},
number={1},
pages={63--73},
year={2007},
publisher={Elsevier}
}
@article{he2012investigation,
title={An investigation on the feasibility of cross-project defect prediction},
author={He, Zhimin and Shu, Fengdi and Yang, Ye and Li, Mingshu and Wang, Qing},
journal={Automated Software Engineering},
volume={19},
number={2},
pages={167--199},
year={2012},
publisher={Springer}
}
@inproceedings{menzies2011local,
title={Local vs. global models for effort estimation and defect prediction},
author={Menzies, Tim and Butcher, Andrew and Marcus, Andrian and Zimmermann, Thomas and Cok, David},
booktitle={Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering},
pages={343--351},
year={2011},
organization={IEEE Computer Society}
}
@article{turhan09,
title={On the relative value of cross-company and within-company data for defect prediction},
author={Turhan, Burak and Menzies, Tim and Bener, Ay{\c{s}}e B and Di Stefano, Justin},
journal={Empirical Software Engineering},
volume={14},
number={5},
pages={540--578},
year={2009},
publisher={Springer}
}
@ARTICLE{briand02,
author={L. C. Briand and W. L. Melo and J. Wust},
journal={IEEE Transactions on Software Engineering},
title={Assessing the applicability of fault-proneness models across object-oriented software projects},
year={2002},
volume={28},
number={7},
pages={706-720},
keywords={cost-benefit analysis;object-oriented programming;project management;software development management;software metrics;software reliability;splines (mathematics);statistical analysis;MARS;a-priori unknown functional form;cost-benefit model;cross-validation;economically viable tools;empirical validation;exploratory analysis technique;fault detection;fault probabilities;fault-proneness model applicability assessment;faulty class prediction;mid-size Java systems;multivariate adaptive regression splines;object-oriented software projects;organizational change;organizational learning;software design metrics;software development projects;software validation;software verification;system class ranking;Accuracy;Computer Society;Economic forecasting;Environmental economics;Fault detection;Java;Logistics;Mars;Object oriented modeling;Predictive models},
doi={10.1109/TSE.2002.1019484},
ISSN={0098-5589},
month={Jul},}
@inproceedings{rahman12,
author = {Rahman, Foyzur and Posnett, Daryl and Devanbu, Premkumar},
title = {Recalling the "Imprecision" of Cross-project Defect Prediction},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
series = {FSE '12},
year = {2012},
isbn = {978-1-4503-1614-9},
location = {Cary, North Carolina},
pages = {61:1--61:11},
articleno = {61},
numpages = {11},
url = {http://doi.acm.org/10.1145/2393596.2393669},
doi = {10.1145/2393596.2393669},
acmid = {2393669},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {empirical software engineering, fault prediction, inspection},
}
@inproceedings{Xu15a,
author = {Xu, Tianyin and Jin, Long and Fan, Xuepeng and Zhou, Yuanyuan and Pasupathy, Shankar and Talwadker, Rukma},
title = {Hey, You Have Given Me Too Many Knobs!: Understanding and Dealing with Over-designed Configuration in System Software},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
series = {ESEC/FSE 2015},
year = {2015},
isbn = {978-1-4503-3675-8},
location = {Bergamo, Italy},
pages = {307--319},
numpages = {13},
url = {http://doi.acm.org/10.1145/2786805.2786852},
doi = {10.1145/2786805.2786852},
acmid = {2786852},
publisher = {ACM},
address = {New York, NY, USA},
keywords = {Complexity, Configuration, Difficulty, Error, Navigation, Parameter, Simplification},
}

@inproceedings{posnet11,
abstract = {Software systems are decomposed hierarchically, for example, into modules, packages and files. This hierarchical decomposition has a profound influence on evolvability, maintainability and work assignment. Hierarchical decomposition is thus clearly of central concern for empirical software engineering researchers; but it also poses a quandary. At what level do we study phenomena, such as quality, distribution, collaboration and productivity? At the level of files? packages? or modules? How does the level of study affect the truth, meaning, and relevance of the findings? In other fields it has been found that choosing the wrong level might lead to misleading or fallacious results. Choosing a proper level, for study, is thus vitally important for empirical software engineering research; but this issue hasn't thus far been explicitly investigated. We describe the related idea of ecological inference and ecological fallacy from sociology and epidemiology, and explore its relevance to empirical software engineering; we also present some case studies, using defect and process data from 18 open source projects to illustrate the risks of modeling at an aggregation level in the context of defect prediction, as well as in hypothesis testing.},
author = {Posnett, Daryl and Filkov, Vladimir and Devanbu, Premkumar},
booktitle = {2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)},
doi = {10.1109/ASE.2011.6100074},
file = {:Users/rkrsn/Documents/Mendeley Desktop//Posnett, Filkov, Devanbu - Ecological inference in empirical software engineering.pdf:pdf},
isbn = {978-1-4577-1639-3},
issn = {1938-4300},
mendeley-groups = {ASE 2016},
month = {nov},
pages = {362--371},
publisher = {IEEE},
title = {{Ecological inference in empirical software engineering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6100074},
year = {2011}
}
@inproceedings{yang11,
abstract = {Background: Continuously calibrated and validated parametric models are necessary for realistic software estimates. However, in practice, variations in model adoption and usage patterns introduce a great deal of local bias in the resultant historical data. Such local bias should be carefully examined and addressed before the historical data can be used for calibrating new versions of parametric models. Aims: In this study, we aim at investigating the degree of such local bias in a cross-company historical dataset, and assessing its impacts on parametric estimation model's performance. Method: Our study consists of three parts: 1) defining a method for measuring and analyzing the local bias associated with individual organization data subset in the overall dataset; 2) assessing the impacts of local bias on the estimation performance of COCOMO II 2000 model; 3) performing a correlation analysis to verify that local bias can be harmful to the performance of a parametric estimation model. Results: Our results show that the local bias negatively impacts the performance of parametric model. Our measure of local bias has a positive correlation with the performance by statistical importance. Conclusion: Local calibration by using the whole multi-company data would get worse performance. The influence of multi-company data could be defined by local bias and be measured by our method.},
address = {New York, New York, USA},
author = {Yang, Ye and Xie, Lang and He, Zhimin and Li, Qi and Nguyen, Vu and Boehm, Barry and Valerdi, Ricardo},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering - Promise '11},
doi = {10.1145/2020390.2020404},
file = {:Users/rkrsn/Documents/Mendeley Desktop//Yang et al. - Local bias and its impacts on the performance of parametric estimation models.pdf:pdf},
isbn = {9781450307093},
keywords = {accuracy indicator,all or part of,effort estimation,local bias,or hard copies of,parametric model,permission to make digital,this work for},
mendeley-groups = {ASE 2016},
pages = {1--10},
publisher = {ACM Press},
title = {{Local bias and its impacts on the performance of parametric estimation models}},
url = {http://dl.acm.org/citation.cfm?doid=2020390.2020404},
year = {2011}
}
@inproceedings{linares2015optimizing,
title={Optimizing energy consumption of guis in android apps: A multi-objective approach},
author={Linares-V{\'a}squez, Mario and Bavota, Gabriele and C{\'a}rdenas, Carlos Eduardo Bernal and Oliveto, Rocco and Di Penta, Massimiliano and Poshyvanyk, Denys},
booktitle={Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages={143--154},
year={2015},
organization={ACM}
}
@inproceedings{me12d,
abstract = {Abstract—Data miners can infer rules showing how to improve either (a) the effort estimates of a project or (b) the defect predictions of a software module. Such studies often exhibit conclusion instability regarding what is the most effective action for different projects or modules. This instability can be explained by data heterogeneity. We show that effort and defect data contain many local regions with markedly different properties to the global space. In other words, what appears to be useful in a global context is often irrelevant for particular local contexts. This result raises questions about the generality of conclusions from empirical SE. At the very least, SE researchers should test if their supposedly general conclusions are valid within subsets of their data. At the very most, empirical SE should become a search for local regions with similar properties (and conclusions should be constrained to just those regions).},
author = {Menzies, Tim and Butcher, Andrew and Marcus, Andrian and Zimmermann, Thomas and Cok, David},
booktitle = {2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)},
doi = {10.1109/ASE.2011.6100072},
file = {:Users/rkrsn/Documents/Mendeley Desktop/Menzies et al. - Local vs. global models for effort estimation and defect prediction.pdf:pdf},
isbn = {978-1-4577-1639-3},
keywords = {Data mining,I,defect/effort estimation,empirical SE.,validation},
mendeley-groups = {ASE 2015 Lit},
month = {nov},
pages = {343--351},
publisher = {IEEE},
title = {{Local vs. global models for effort estimation and defect prediction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6100072},
year = {2011}
}
@misc{Hassan17,
author="A. Hassan",
title="Remarks made during a presentation to the UCL Crest Open Workshop",
month="March",
year=2017
}
@inproceedings{Bettenburg2012,
abstract = {Much research energy in software engineering is focused on the creation of effort and defect prediction models. Such models are important means for practitioners to judge their current project situation, optimize the allocation of their resources, and make informed future decisions. However, software engineering data contains a large amount of variability. Recent research demonstrates that such variability leads to poor fits of machine learning models to the underlying data, and suggests splitting datasets into more fine-grained subsets with similar properties. In this paper, we present a comparison of three different approaches for creating statistical regression models to model and predict software defects and development effort. Global models are trained on the whole dataset. In contrast, local models are trained on subsets of the dataset. Last, we build a global model that takes into account local characteristics of the data. We evaluate the performance of these three approaches in a case study on two defect and two effort datasets. We find that for both types of data, local models show a significantly increased fit to the data compared to global models. The substantial improvements in both relative and absolute prediction errors demonstrate that this increased goodness of fit is valuable in practice. Finally, our experiments suggest that trends obtained from global models are too general for practical recommendations. At the same time, local models provide a multitude of trends which are only valid for specific subsets of the data. Instead, we advocate the use of trends obtained from global models that take into account local characteristics, as they combine the best of both worlds.},
author = {Bettenburg, Nicolas and Nagappan, Meiyappan and Hassan, Ahmed E.},
booktitle = {2012 9th IEEE Working Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2012.6224300},
file = {:Users/rkrsn/Documents/Mendeley Desktop/Bettenburg, Nagappan, Hassan - Think locally, act globally Improving defect and effort prediction models(2).pdf:pdf},
isbn = {978-1-4673-1761-0},
issn = {21601852},
keywords = {models,software metrics,techniques},
mendeley-groups = {ASE 2016},
month = {jun},
pages = {60--69},
publisher = {IEEE},
title = {{Think locally, act globally: Improving defect and effort prediction models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6224300},
year = {2012}
}
@article{ma12,
abstract = {Context: Software defect prediction studies usually built models using within-company data, but very few focused on the prediction models trained with cross-company data. It is difficult to employ these models which are built on the within-company data in practice, because of the lack of these local data repositories. Recently, transfer learning has attracted more and more attention for building classifier in target domain using the data from related source domain. It is very useful in cases when distributions of training and test instances differ, but is it appropriate for cross-company software defect prediction? Objective: In this paper, we consider the cross-company defect prediction scenario where source and target data are drawn from different companies. In order to harness cross company data, we try to exploit the transfer learning method to build faster and highly effective prediction model. Method: Unlike the prior works selecting training data which are similar from the test data, we proposed a novel algorithm called Transfer Naive Bayes (TNB), by using the information of all the proper features in training data. Our solution estimates the distribution of the test data, and transfers cross-company data information into the weights of the training data. On these weighted data, the defect prediction model is built. Results: This article presents a theoretical analysis for the comparative methods, and shows the experiment results on the data sets from different organizations. It indicates that TNB is more accurate in terms of AUC (The area under the receiver operating characteristic curve), within less runtime than the state of the art methods. Conclusion: It is concluded that when there are too few local training data to train good classifiers, the useful knowledge from different-distribution training data on feature level may help. We are optimistic that our transfer learning method can guide optimal resource allocation strategies, which may reduce software testing cost and increase effectiveness of software testing process. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Ma, Ying and Luo, Guangchun and Zeng, Xue and Chen, Aiguo},
doi = {10.1016/j.infsof.2011.09.007},
file = {:Users/rkrsn/Documents/Mendeley Desktop//Ma et al. - Transfer learning for cross-company software defect prediction - 2012.pdf:pdf},
issn = {09505849},
journal = {Inf. Softw. Technol.},
keywords = {Different distribution,Machine learning,Naive Bayes,Software defect prediction,Transfer learning},
month = {mar},
number = {3},
pages = {248--256},
title = {{Transfer learning for cross-company software defect prediction}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950584911001996},
volume = {54},
year = {2012}
}
@inproceedings{Nam2015,
address = {New York, New York, USA},
author = {Nam, Jaechang and Kim, Sunghun},
booktitle = {Proc. 2015 10th Jt. Meet. Found. Softw. Eng. - ESEC/FSE 2015},
doi = {10.1145/2786805.2786814},
file = {:Users/rkrsn/Documents/Mendeley Desktop/Nam, Kim - Heterogeneous defect prediction - 2015.pdf:pdf},
isbn = {9781450336758},
keywords = {defect prediction,heterogeneous metrics,quality assurance},
mendeley-groups = {ASE 2016},
pages = {508--519},
publisher = {ACM Press},
title = {{Heterogeneous defect prediction}},
url = {http://dl.acm.org/citation.cfm?doid=2786805.2786814},
year = {2015}
}
@article{menzies2010defect,
	title={Defect prediction from static code features: current results, limitations, 
	new approaches},
	author={Menzies, Tim and Milton, Zach and Turhan, Burak and Cukic, Bojan 
	and Jiang, Yue and Bener, Ay{\c{s}}e},
	journal={Automated Software Engineering},
	volume={17},
	number={4},
	pages={375--407},
	year={2010},
	publisher={Springer}
}
@article{elish2008predicting,
	title={Predicting defect-prone software modules using support vector 
	machines},
	author={Elish, Karim O and Elish, Mahmoud O},
	journal={JSS},
	volume={81},
	number={5},
	pages={649--660},
	year={2008},
	publisher={Elsevier}
}
@article{gondra2008applying,
	title={Applying machine learning to software fault-proneness prediction},
	author={Gondra, Iker},
	journal={Journal of Systems and Software},
	volume={81},
	number={2},
	pages={186--195},
	year={2008},
	publisher={Elsevier}
}
@article{radjenovic2013software,
	title={Software fault prediction metrics: A systematic literature review},
	author={Radjenovi{\'c}, Danijel and Heri{\v{c}}ko, Marjan and Torkar, Richard 
	and {\v{Z}}ivkovi{\v{c}}, Ale{\v{s}}},
	journal={Information and Software Technology},
	volume={55},
	number={8},
	pages={1397--1418},
	year={2013},
	publisher={Elsevier}
}
@article{jiang2008techniques,
	title={Techniques for evaluating fault prediction models},
	author={Jiang, Yue and Cukic, Bojan and Ma, Yan},
	journal={Empirical Software Engineering},
	volume={13},
	number={5},
	pages={561--595},
	year={2008},
	publisher={Springer}
}
@article{wang2013using,
	title={Using class imbalance learning for software defect prediction},
	author={Wang, Shuo and Yao, Xin},
	journal={IEEE Transactions on Reliability},
	volume={62},
	number={2},
	pages={434--443},
	year={2013},
	publisher={IEEE}
}
@article{li2012sample,
	title={Sample-based software defect prediction with active and 
	semi-supervised learning},
	author={Li, Ming and Zhang, Hongyu and Wu, Rongxin and Zhou, Zhi-Hua},
	journal={Automated Software Engineering},
	volume={19},
	number={2},
	pages={201--230},
	year={2012},
	publisher={Springer}
}
@inproceedings{khoshgoftaar2010attribute,
	title={Attribute selection and imbalanced data: Problems in software defect 
	prediction},
	author={Khoshgoftaar, Taghi M and Gao, Kehan and Seliya, Naeem},
	booktitle={Tools with Artificial Intelligence (ICTAI), 2010 22nd IEEE 
	International Conference on},
	volume={1},
	pages={137--144},
	year={2010},
	organization={IEEE}
}
@inproceedings{jiang2009variance,
	title={Variance analysis in software fault prediction models},
	author={Jiang, Yue and Lin, Jie and Cukic, Bojan and Menzies, Tim},
	booktitle={Software Reliability Engineering, 2009. ISSRE'09. 20th 
	International Symposium on},
	pages={99--108},
	year={2009},
	organization={IEEE}
}
@inproceedings{ghotra2015revisiting,
	title={Revisiting the impact of classification techniques on the performance 
	of defect prediction models},
	author={Ghotra, Baljinder and McIntosh, Shane and Hassan, Ahmed E},
	booktitle={37th ICSE-Volume 1},
	pages={789--800},
	year={2015},
	organization={IEEE Press}
}
@inproceedings{jiang2008can,
	title={Can data transformation help in the detection of fault-prone modules?},
	author={Jiang, Yue and Cukic, Bojan and Menzies, Tim},
	booktitle={Proceedings of the 2008 workshop on Defects in large software 
	systems},
	pages={16--20},
	year={2008},
	organization={ACM}
}
@inproceedings{tantithamthavorn2016automated,
	title={Automated parameter optimization of classification techniques for 
	defect prediction models},
	author={Tantithamthavorn, Chakkrit and McIntosh, Shane and Hassan, 
	Ahmed E and Matsumoto, Kenichi},
	booktitle={ICSE 2016},
	pages={321--332},
	year={2016},
	organization={ACM}
}
@article{fu2016tuning,
	title={Tuning for software analytics: Is it really necessary?},
	author={Fu, Wei and Menzies, Tim and Shen, Xipeng},
	journal={IST},
	volume={76},
	pages={135--146},
	year={2016},
	publisher={Elsevier}
}
@article{agarwal17,
	author    = {Amritanshu Agrawal and
	Tim Menzies},
	title     = {"Better Data" is Better than "Better Data Miners" (Benefits of Tuning
	{SMOTE} for Defect Prediction)},
	journal   = {CoRR},
	volume    = {abs/1705.03697},
	year      = {2017},
	url       = {http://arxiv.org/abs/1705.03697},
	archivePrefix = {arXiv},
	eprint    = {1705.03697},
	timestamp = {Wed, 07 Jun 2017 14:41:24 +0200},
	biburl    = {http://dblp.org/rec/bib/journals/corr/AgrawalM17},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}
@inproceedings{mende2009revisiting,
	title={Revisiting the evaluation of defect prediction models},
	author={Mende, Thilo and Koschke, Rainer},
	booktitle={Proceedings of the 5th International Conference on Predictor 
	Models in Software Engineering},
	pages={7},
	year={2009},
	organization={ACM}
}
@article{DAmbros2012,
abstract = {Reliably predicting software defects is one of the holy grails of software engineering. Researchers have devised and implemented a plethora of defect/bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available dataset consisting of several software systems, and provide an extensive comparison of well-known bug prediction approaches, together with novel approaches we devised. We evaluate the performance of the approaches using different performance indicators: classification of entities as defect-prone or not, ranking of the entities, with and without taking into account the effort to review an entity. We performed three sets of experiments aimed at (1) comparing the approaches across different systems, (2) testing whether the differences in performance are statistically significant, and (3) investigating the stability of approaches across different learners. Our results indicate that, while some approaches perform better than others in a statistically significant manner, external validity in defect prediction is still an open problem, as generalizing results to different contexts/learners proved to be a partially unsuccessful endeavor.},
author = {D'Ambros, Marco and Lanza, Michele and Robbes, Romain},
doi = {10.1007/s10664-011-9173-9},
isbn = {1066401191739},
issn = {1382-3256},
journal = {Empir. Softw. Eng.},
keywords = {Change metrics,Defect prediction,Source code metrics},
month = {aug},
number = {4-5},
pages = {531--577},
title = {{Evaluating defect prediction approaches: a benchmark and an extensive comparison}},
url = {http://link.springer.com/10.1007/s10664-011-9173-9},
volume = {17},
year = {2012}
}
@inproceedings{Wu2011,
abstract = {Software defect information, including links between bugs and committed changes, plays an important role in software maintenance such as measuring quality and predicting defects. Usually, the links are automatically mined from change logs and bug reports using heuristics such as searching for specific keywords and bug IDs in change logs. However, the accuracy of these heuristics depends on the quality of change logs. Bird et al. found that there are many missing links due to the absence of bug references in change logs. They also found that the missing links lead to biased defect information, and it affects defect prediction performance. We manually inspected the explicit links, which have explicit bug IDs in change logs and observed that the links exhibit certain features. Based on our observation, we developed an automatic link recovery algorithm, ReLink, which automatically learns criteria of features from explicit links to recover missing links. We applied ReLink to three open source projects. ReLink reliably identified links with 89{\%} precision and 78{\%} recall on average, while the traditional heuristics alone achieve 91{\%} precision and 64{\%} recall. We also evaluated the impact of recovered links on software maintainability measurement and defect prediction, and found the results of ReLink yields significantly better accuracy than those of traditional heuristics. {\textcopyright} 2011 ACM.},
address = {New York, New York, USA},
author = {Wu, Rongxin and Zhang, Hongyu and Kim, Sunghun and Cheung, Shing-Chi},
booktitle = {Proc. 19th ACM SIGSOFT Symp. 13th Eur. Conf. Found. Softw. Eng. - SIGSOFT/FSE '11},
doi = {10.1145/2025113.2025120},
isbn = {9781450304436},
issn = {9781450304436},
keywords = {Bugs,Changes,Computer software maintenance,Data quality,Defects,Forecasting,Maintainability,Mining software repository,Missing links,Program debugging,Recovery,Software engineering,and bug ids in,bird et al,bug,change logs,found that there are,however,logs,many missing links due,references in change logs,the accuracy of,the missing links,the quality of change,these heuristics depends on,they also found that,to the absence of},
pages = {15},
year={2011},
publisher = {ACM Press},
title = {{ReLink}},
}
@inproceedings{Jureczko2010,
abstract = {Background: This paper describes an analysisthat was conducted on newly collected repository with 92 versions of 38 proprietary, open-source and academic projects. A preliminary perfomed before showed the need for a further in-depth analysis study in order to identify project clusters. Aims: The goal of this research is to perform clustering on software projects in order to identify groups of software projects with similar characteristic from the defect prediction point of view. One defect prediction model should work well for all projects that belong to such group. The existence of those groups was investigated with statistical tests and by comparing the mean value of prediction efficiency. Method: Hierarchical and k-means clustering, as well obtained clusters were investigated with as Kohonen's neural network was used to find groups of similar projects. The the discriminant analysis. For each of the identified group a statistical analysis has been conducted in order to distinguish whether this group really exists. Two defect prediction models were created for each of the identified groups. The first one was based on the projects that belong to a given group, and the second one - on all the projects. Then, both models were applied to all versions of projects from the investigated group. If the predictions from the model based on projects that belong to the identified group are significantly better than the all-projects model (the mean values were compared and statistical tests were used), we conclude that the group really exists. Results: Six different clusters were identified and the existence of two of them was statistically proven: 1) cluster proprietary B – T=19, p=0.035, r=0.40; 2) cluster proprietary/open – t(17)=3.18, p=0.05, r=0.59. The obtained effect sizes (r) represent large effects according to Cohen's benchmark, which is a substantial finding. Conclusions: The two identified clusters were described and compared with results obtained by other researchers. The results of this work makes next step towards defining formal methods of reuse defect prediction models by identifying groups of projects within which the same defect prediction model may be used. Furthermore, a method of clustering was suggested and applied.},
address = {New York, New York, USA},
author = {Jureczko, Marian and Madeyski, Lech},
booktitle = {Proc. 6th Int. Conf. Predict. Model. Softw. Eng. - PROMISE '10},
doi = {10.1145/1868328.1868342},
isbn = {9781450304047},
keywords = {clustering,defect prediction,design metrics,size metrics},
pages = {1},
publisher = {ACM Press},
title = {{Towards identifying software project clusters with regard to defect prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1868328.1868342},
year = {2010}
}
@article{basili1996validation,
title={A validation of object-oriented design metrics as quality indicators},
author={Basili, Victor R and Briand, Lionel C and Melo, Walc{\'e}lio L},
journal={Software Engineering, IEEE Transactions on},
volume={22},
number={10},
pages={751--761},
year={1996},
publisher={IEEE}
}
@article{ohlsson1996predicting,
title={Predicting fault-prone software modules in telephone switches},
author={Ohlsson, Niclas and Alberg, Hans},
journal={Software Engineering, IEEE Transactions on},
volume={22},
number={12},
pages={886--894},
year={1996},
publisher={IEEE}
}
@inproceedings{kim2011dealing,
title={Dealing with noise in defect prediction},
author={Kim, Sunghun and Zhang, Hongyu and Wu, Rongxin and Gong, Liang},
booktitle={Software Engineering (ICSE), 2011 33rd International Conference on},
pages={481--490},
year={2011},
organization={IEEE}
}

@article{vargha2000,
title={A critique and improvement of the CL common language effect size statistics of McGraw and Wong},
author={Vargha, Andr{\'a}s and Delaney, Harold D},
journal={Journal of Educational and Behavioral Statistics},
volume={25},
number={2},
pages={101--132},
year={2000},
publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@inproceedings{leech2002call,
title={A Call for Greater Use of Nonparametric Statistics.},
author={Leech, Nancy L and Onwuegbuzie, Anthony J},
year={2002},
publisher={ERIC},
booktitle={Annual Meeting of the Mid-South Educational Research Association}
}

@article{poulding10,
title={Efficient software verification: Statistical testing using automated search},
author={Poulding, Simon and Clark, John A},
journal={IEEE Transactions on Software Engineering},
volume={36},
number={6},
pages={763--777},
year={2010},
publisher={IEEE}
}

@article{Shepperd2013,
abstract = {Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset and generally insufficient detail documented on dataset preprocessing. Conclusions--It is recommended that researchers 1) indicate the provenance of the datasets they use, 2) report any preprocessing in sufficient detail to enable meaningful replication, and 3) invest effort in understanding the data prior to applying machine learners.},
author = {Shepperd, Martin and Song, Qinbao and Sun, Zhongbin and Mair, Carolyn},
doi = {10.1109/TSE.2013.11},
issn = {0098-5589},
journal = {IEEE Trans. Softw. Eng.},
keywords = {Empirical software engineering,data quality,defect prediction,machine learning},
month = {sep},
number = {9},
pages = {1208--1215},
title = {{Data Quality: Some Comments on the NASA Software Defect Datasets}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6464273},
volume = {39},
year = {2013}
}

@book{Cohen1995,
title={Empirical methods for artificial intelligence},
author={Cohen, Paul R},
volume={139},
year={1995},
isbn = {0262032252},
pages = {405},
url = {http://www.3w-bilderbuch.de/CohenPaulR/CohenPaulR0262032252.htm},
publisher={MIT press Cambridge}
}

@article{lessmann08,
abstract = {Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.},
author = {Lessmann, Stefan and Baesens, Bart and Mues, C. and Pietsch, S.},
doi = {10.1109/TSE.2008.35},
isbn = {00985589 (ISSN)},
issn = {0098-5589},
journal = {IEEE Trans. Softw. Eng.},
keywords = {Complexity measures,Data mining,Formal methods,Statistical methods,benchmark testing,benchmarking classification models,code attributes,fault-prone modules,metric-based classification,predictive classification models,proprietary data sets,software defect prediction,software quality,statistical testing,statistical testing procedures,testing efficiency},
month = {jul},
number = {4},
pages = {485--496},
title = {{Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4527256},
volume = {34},
year = {2008}
}
@inproceedings{Pelayo2007,
abstract = {Due to the tremendous complexity and sophistication of software, improving software reliability is an enormously difficult task. We study the software defect prediction problem, which focuses on predicting which modules will experience a failure during operation. Numerous studies have applied machine learning to software defect prediction; however, skewness in defect-prediction datasets usually undermines the learning algorithms. The resulting classifiers will often never predict the faulty minority class. This problem is well known in machine learning and is often referred to as learning from unbalanced datasets. We examine stratification, a widely used technique for learning unbalanced data that has received little attention in software defect prediction. Our experiments are focused on the SMOTE technique, which is a method of over-sampling minority-class examples. Our goal is to determine if SMOTE can improve recognition of defect-prone modules, and at what cost. Our experiments demonstrate that after SMOTE resampling, we have a more balanced classification. We found an improvement of at least 23{\%} in the average geometric mean classification accuracy on four benchmark datasets.},
author = {Pelayo, Lourdes and Dick, Scott},
booktitle = {NAFIPS 2007 - 2007 Annu. Meet. North Am. Fuzzy Inf. Process. Soc.},
doi = {10.1109/NAFIPS.2007.383813},
isbn = {1-4244-1213-7},
month = {jun},
pages = {69--72},
publisher = {IEEE},
title = {{Applying Novel Resampling Strategies To Software Defect Prediction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4271036},
year = {2007}
}
@article{Chawla2002,
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally rep- resented. Often real-world data sets are predominately composed of “normal” examples with only a small percentage of “abnormal” or “interesting” examples. It is also the case that the cost of misclassifying an abnormal (interesting)example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (nor- mal)class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal)class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space)than only under-sampling the majority class. This paper also shows that a combination of ourmethod of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space)than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC)and the ROC convex hull strategy},
archivePrefix = {arXiv},
arxivId = {1106.1813},
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
doi = {10.1613/jair.953},
eprint = {1106.1813},
isbn = {013805326X},
issn = {10769757},
journal = {J. Artif. Intell. Res.},
pmid = {18190633},
title = {{SMOTE: Synthetic minority over-sampling technique}},
volume = {16},
year = {2002}
}
@INPROCEEDINGS{ma07,
author={Y. Ma and B. Cukic},
booktitle={Predictor Models in Software Engineering, 2007. PROMISE'07: ICSE Workshops 2007. International Workshop on},
title={Adequate and Precise Evaluation of Quality Models in Software Engineering Studies},
year={2007},
pages={1-1},
keywords={software engineering;statistical analysis;fault-proneness prediction;performance assessment;predictive models;quality models evaluation;software engineering studies;statistical techniques;Classification tree analysis;Computer science;Costs;Genetic algorithms;Logistics;Neural networks;Predictive models;Software engineering;Software metrics;Software quality},
doi={10.1109/PROMISE.2007.1},
month={May},}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the corre- lation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth Interna- tional conference, ∗∗∗, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression. Keywords:},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1023\%2FA\%3A1010933404324},
author = {Breiman, L},
doi = {10.1023/A:1010933404324},
eprint = {/dx.doi.org/10.1023\%2FA\%3A1010933404324},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine learning},
keywords = {classification,ensemble,regression},
pages = {5--32},
pmid = {21816105},
primaryClass = {http:},
title = {{Random forests}},
url = {http://link.springer.com/article/10.1023/A:1010933404324},
year = {2001}
}

@article{mittas13,
author =	{Nikolaos Mittas and Lefteris Angelis},
title =	{Ranking and Clustering Software Cost Estimation
Models through a Multiple Comparisons Algorithm},
journal =	{IEEE Trans. Software Eng.},
volume =	39,
number =	4,
year =	2013,
pages =	{537-551},
}

@book{efron93,
author =	"Efron, Bradley and Tibshirani, Robert J",
title =	"An introduction to the bootstrap",
publisher =	"Chapman and Hall",
address =	"London",
series =	"Mono. Stat. Appl. Probab.",
year =	1993,
}

@INPROCEEDINGS{arcuri11,
author={Arcuri, A. and Briand, L.},
booktitle={ICSE'11},
title={A practical guide for using statistical tests to assess randomized algorithms in software engineering},
year={2011},
pages={1-10}}

@article{shepperd12a,
author =	 {Martin J. Shepperd and Steven G. MacDonell},
title =	 {Evaluating prediction systems in software project
estimation},
journal =	 {Information {\&} Software Technology},
volume =	 54,
number =	 8,
year =	 2012,
pages =	 {820-827},
}

@article{kampenes07,
author =	 {Vigdis By Kampenes and Tore Dyb{\aa} and Jo Erskine
Hannay and Dag I. K. Sj{\o}berg},
title =	 {A systematic review of effect size in software
engineering experiments},
journal =	 {Information {\&} Software Technology},
volume =	 49,
number =	 {11-12},
year =	 2007,
pages =	 {1073-1086},
}

@inproceedings{ekanayake2009tracking,
  title={Tracking concept drift of software projects using defect prediction quality},
  author={Ekanayake, Jayalath and Tappolet, Jonas and Gall, Harald C and Bernstein, Abraham},
  booktitle={Mining Software Repositories, 2009. MSR'09. 6th IEEE International Working Conference on},
  pages={51--60},
  year={2009},
  organization={IEEE}
}


@inproceedings{Kocaguneli2013:ep,
author = {Kocaguneli, Ekrem and Zimmermann, Thomas and Bird, Christian and Nagappan, Nachiappan and Menzies, Tim},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1109/ICSE.2013.6606637},
isbn = {9781467330763},
issn = {02705257},
pages = {882--890},
title = {{Distributed development considered harmful?}},
year = {2013}
}
@inproceedings{bachmann10,
title={The missing links: bugs and bug-fix commits},
author={Bachmann, Adrian and Bird, Christian and Rahman, Foyzur and Devanbu, Premkumar and Bernstein, Abraham},
booktitle={Proceedings of the eighteenth ACM SIGSOFT international symposium on Foundations of software engineering},
pages={97--106},
year={2010},
organization={ACM}

}
@inproceedings{bird09,
title={Fair and balanced?: bias in bug-fix datasets},
author={Bird, Christian and Bachmann, Adrian and Aune, Eirik and Duffy, John and Bernstein, Abraham and Filkov, Vladimir and Devanbu, Premkumar},
booktitle={Proceedings of the the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering},
pages={121--130},
year={2009},
organization={ACM}
}
@inproceedings{kim11,
title={Dealing with noise in defect prediction},
author={Kim, Sunghun and Zhang, Hongyu and Wu, Rongxin and Gong, Liang},
booktitle={Software Engineering (ICSE), 2011 33rd International Conference on},
pages={481--490},
year={2011},
organization={IEEE}
}

%% Created for Andrew Butcher at 2011-09-09 14:16:01 -0400


%% Saved with string encoding Unicode (UTF-8)




@Article{briand02,
author = {Briand, Lionel Claude and Wuest, J},
title = {Empirical Studies of Quality Models in Object-Oriented Systems},
year = {2002},
journal = {Advances in Computers},
volume = {56},
number = {0},
note = {Edited by M. Zelkowitz}
}



@article{ref28,
Author = {G. Succi},
Date-Added = {2011-09-09 14:15:22 -0400},
Date-Modified = {2011-09-09 14:16:01 -0400},
Journal = {Journal of Systems and Software},
Month = {January},
Number = {1},
Pages = {1-12},
Title = {Practical assessment of the models for identification of defect-prone classes in object-oriented commercial systems using design metrics,},
Volume = {65},
Year = {2003}}

@article{ref27,
Author = {J. Xu and D. Ho and L.  Capretz},
Date-Added = {2011-09-09 14:14:16 -0400},
Date-Modified = {2011-09-09 14:15:19 -0400},
Journal = {Journal of Computer Science},
Month = {July},
Pages = {571-577},
Title = {An Empirical Validation of Object-Oriented Design Metrics for Fault Prediction},
Year = {2008}}

@article{ref26,
Author = {M. Thapaliyal and G. Verma},
Date-Added = {2011-09-09 14:13:13 -0400},
Date-Modified = {2011-09-09 14:14:13 -0400},
Journal = {International Journal of Computer Applications},
Title = {Software Defects and Object Oriented Metrics-An Empirical Analysis},
Volume = {9/5},
Year = {2010}}

@article{ref25,
Author = {K. el Emam and S. Benlarbi and N. Goel and S.  Rai},
Date-Added = {2011-09-09 14:12:29 -0400},
Date-Modified = {2011-09-09 14:13:11 -0400},
Journal = {Software Engineering, IEEE Transactions},
Number = {7},
Pages = {630-650},
Title = {The Confounding Effect of Class Size on the Validity of Object-Oriented Metrics},
Volume = {27},
Year = {2001}}

@article{ref24,
Author = {D. Glasberg and  K. el~Emam and W. Memo
and N. Madhavji},
Date-Added = {2011-09-09 14:10:27 -0400},
Date-Modified = {2011-09-09 14:12:27 -0400},
Journal = {NRC 44146},
Title = {Validating object-oriented design metrics on a
commercial {JAVA} application},
Year = {2000}}

@article{ref23,
Author = {Y. Singh and A. Kaur and R. Malhotra},
Date-Added = {2011-09-09 14:09:53 -0400},
Date-Modified = {2011-09-09 14:10:24 -0400},
Journal = {Software Quality Journal},
Number = {1},
Pages = {3-35},
Title = {Empirical validation of object-oriented metrics for predicting fault proneness models},
Volume = {18},
Year = {2010}}

@article{ref22,
Author = {R. Shatnawi},
Date-Added = {2011-09-09 14:08:58 -0400},
Date-Modified = {2011-09-09 14:09:51 -0400},
Journal = {IEEE Transactions on Software Engineering},
Number = {2},
Pages = {216-225},
Title = {A Quantitative Investigation of the Acceptable Risk Levels of Object-Oriented Metrics in Open-Source Systems},
Volume = {36},
Year = {2010}}

@article{ref21,
Author = {M. English and C. Exton and I. Rigon and B. Cleary},
Date-Added = {2011-09-09 14:08:24 -0400},
Date-Modified = {2011-09-09 14:08:56 -0400},
Journal = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering},
Number = {1-11},
Title = {Fault detection and prediction in an open-source software project},
Year = {2009}}

@article{ref20,
Author = {A. Janes and M. Scotto and W. Pedrycz and B. Russo and M. Stefanovic and G. Succi},
Date-Added = {2011-09-09 14:07:38 -0400},
Date-Modified = {2011-09-09 14:08:22 -0400},
Journal = {Information Sciences},
Number = {24},
Pages = {3711-3734},
Title = {Identification of defect-prone classes in telecommunication software systems using design metrics},
Volume = {176},
Year = {2006}}

@article{ref19,
Author = {G. Denaro and  L. Lavazza and M. Pezze},
Date-Added = {2011-09-09 14:06:29 -0400},
Date-Modified = {2011-09-09 14:07:35 -0400},
Journal = {The 5th CaberNet Plenary Workshop, Porto Santo, Madeira Archipelago, Portugal},
Title = {An empirical evaluation of object oriented metrics in industrial setting},
Year = {2003}}

@article{ref18,
Author = {M. Thongmak and P. Muenchaisri},
Date-Added = {2011-09-09 14:05:51 -0400},
Date-Modified = {2011-09-09 14:06:23 -0400},
Journal = {Software Engineering Research and Practice},
Pages = {621-627},
Title = {Predicting Faulty Classes using Design Metrics with Discriminant Analysis},
Year = {2003}}

@article{ref17,
Author = {F. Fioravanti and P. Nesi},
Date-Added = {2011-09-09 14:05:14 -0400},
Date-Modified = {2011-09-09 14:05:49 -0400},
Journal = {Software Maintenance and Reengineering, 2001. Fifth European Conference},
Pages = {121-130},
Title = {A study on fault-proneness detection of object-oriented systems},
Year = {2001}}

@article{ref16,
Author = {R. Shatnawi and W. Li},
Date-Added = {2011-09-09 14:04:40 -0400},
Date-Modified = {2011-09-09 14:05:12 -0400},
Journal = {Journal of Systems and Software},
Number = {11},
Pages = {1868-1882},
Title = {The effectiveness of software metrics in identifying error-prone classes in post-release software evolution process},
Volume = {81},
Year = {2008}}

@article{ref15,
Author = {T. Holschuh and M. Pauser and K. Herzig and T. Zimmermann and R. Premraj
and A. Zeller},
Date-Added = {2011-09-09 14:03:26 -0400},
Date-Modified = {2011-09-09 14:04:33 -0400},
Journal = {Software Engineering - Companion Volume, 2009. ICSE-Companion 2009. 31st International Conference},
Pages = {172-181},
Title = {Predicting defects in {SAP} {J}ava code: An experience report},
Year = {2009}}

@article{ref14,
Author = {T. Gyimothy and R. Ferenc and I. Siket},
Date-Added = {2011-09-09 14:02:43 -0400},
Date-Modified = {2011-09-09 14:03:22 -0400},
Journal = {Software Engineering, IEEE Transactions},
Number = {10},
Pages = {897-910},
Title = {Empirical validation of object-oriented metrics on open source software for fault prediction},
Volume = {31},
Year = {2005}}

@article{ref13,
Author = {Y. Zhou and H. Leung},
Date-Added = {2011-09-09 14:02:02 -0400},
Date-Modified = {2011-09-09 14:02:40 -0400},
Journal = {Software Engineering, IEEE Transactions},
Number = {10},
Pages = {771-789},
Title = {Empirical analysis of object-oriented design metrics for predicting high and low severity faults},
Volume = {32},
Year = {2006}}

@article{ref12,
Author = {R. Subramanyam and M. S. Krishnan},
Date-Added = {2011-09-09 14:01:15 -0400},
Date-Modified = {2011-09-09 14:01:59 -0400},
Journal = {IEEE Transactions on Software Engineering},
Month = {April},
Number = {4},
Pages = {297-310},
Title = {Empirical analysis of CK metrics for object-oriented design complexity: implications for software defects},
Volume = {29},
Year = {2003}}

@article{ref11,
Author = {Ping Yu and T. Systa and H. Muller},
Date-Added = {2011-09-09 14:00:24 -0400},
Date-Modified = {2011-09-09 14:01:08 -0400},
Journal = {Sixth European Conference on Software Maintenance and Reengineering},
Pages = {99-107},
Title = {Predicting Fault-Proneness using OO Metrics An Industrial Case Study},
Year = {2002}}

@article{ref10,
Author = {M. Tang and M. Kao and M. Chen},
Date-Added = {2011-09-09 13:59:12 -0400},
Date-Modified = {2011-09-09 14:00:22 -0400},
Journal = {Software Metrics Symposium},
Pages = {242-249},
Title = {An empirical study on object-oriented metrics},
Volume = { Proceedings. Sixth International},
Year = {1999}}

@article{ref09,
Author = {K. el Emam and S. Benlarbi and N. Goel and S. Rai},
Date-Added = {2011-09-09 13:58:33 -0400},
Date-Modified = {2011-09-09 13:59:09 -0400},
Journal = {National Research Council of Canada, NRC/ERB,},
Title = {A validation of object-oriented metrics},
Volume = {1063},
Year = {1999}}

@article{ref08,
Author = {K.  el~Emam and W. Melo and J. Machado},
Date-Added = {2011-09-09 13:57:49 -0400},
Date-Modified = {2011-09-09 13:58:28 -0400},
Journal = {Journal of Systems and Software},
Number = {1},
Pages = {63-75},
Title = {The prediction of faulty classes using object-oriented design metrics},
Volume = {56},
Year = {2001}}

@article{ref07,
Author = {M. Cartwright and M. Shepperd},
Date-Added = {2011-09-09 13:56:53 -0400},
Date-Modified = {2011-09-09 13:57:44 -0400},
Journal = {Software Engineering, IEEE Transactions},
Number = {8},
Pages = {786-796},
Title = {An empirical investigation of an object-oriented software system},
Volume = {26},
Year = {2000}}

@article{ref06,
Author = {L. Briand and J. Wust and H. Lounis},
Date-Added = {2011-09-09 13:56:11 -0400},
Date-Modified = {2011-09-09 13:56:49 -0400},
Journal = {Empirical Software Engineering},
Number = {1},
Pages = {11-58},
Title = {Replicated case studies for investigating quality factors in object-oriented designs},
Volume = {6},
Year = {2001}}

@article{ref05,
Author = {L. Briand and J. Wust and J. Daly and D. Victor Porter},
Date-Added = {2011-09-09 13:55:27 -0400},
Date-Modified = {2011-09-09 13:56:09 -0400},
Journal = {Journal of Systems and Software},
Number = {3},
Pages = {245-273},
Title = {Exploring the relationships between design measures and software quality in object-oriented systems},
Volume = {51},
Year = {2000}}

@article{ref04,
Author = {V.  Basili and L.  Briand and W.  Melo},
Date-Added = {2011-09-09 13:54:38 -0400},
Date-Modified = {2011-09-09 13:55:17 -0400},
Journal = {Software Engineering, IEEE Transactions },
Number = {10},
Pages = {751-761},
Title = {A validation of object-oriented design metrics as quality indicators},
Volume = {22},
Year = {1996}}

@article{ref03,
Author = {E. Arisholm and L. Briand},
Date-Added = {2011-09-09 13:53:32 -0400},
Date-Modified = {2011-09-09 13:54:35 -0400},
Journal = {2006 ACM/IEEE international symposium on Empirical software engineering},
Pages = {17},
Title = {Predicting fault prone components in a {JAVA}
legacy system},
Year = {2006}}

@article{ref02,
Author = {K. Aggmakarwal and Y. Singh and  A. Kaur and R. Malhotra},
Journal = {Software Process: Improvement and Practice},
Month = {January},
Number = {1},
Title = {Empirical analysis for investigating the effect of object-oriented metrics on fault proneness: a replicated case study},
Volume = {14},
Year = {2009}
}

@article{ref01,
Author = {H. Olague and L. Etzkorn and S. Gholston and S. Quattlebaum},
Date-Added = {2011-09-09 13:49:33 -0400},
Date-Modified = {2011-09-09 13:52:23 -0400},
Journal = {Software Engineering, IEEE Transactions},
Number = {6},
Pages = {402-419},
Title = {Empirical Validation of Three Software Metrics Suites to Predict Fault-Proneness of Object-Oriented Classes Developed Using Highly Iterative or Agile Software Development Processes},
Volume = {33},
Year = {2007}}

@INPROCEEDINGS{ref29,
author = {Erich Schikuta and Erich Schikuta},
title = {Grid-Clustering: A Hierarchical Clustering Method for Very Large Data Sets},
booktitle = {In Proceedings 15 th J[nt. Conf. on Pattern Recognition},
year = {1993},
pages = {101--105}
}

@INPROCEEDINGS{Xiao07e.:an,
author = {Lurong Xiao},
title = {E.: An efficient distance calculation method for uncertain objects},
booktitle = {In: Proceedings of 2007 IEEE Symposium on Computational Intelligence and Data Mining (CIDM},
year = {2007}
}

@inproceedings{Tufano2015,
author = {Tufano, Michele and Palomba, Fabio and Bavota, Gabriele and Oliveto, Rocco and {Di Penta}, Massimiliano and {De Lucia}, Andrea and Poshyvanyk, Denys},
booktitle = {2015 IEEE/ACM 37th IEEE Int. Conf. Softw. Eng.},
doi = {10.1109/ICSE.2015.59},
file = {:Users/rkrsn/Documents/Mendeley Desktop/Tufano et al. - When and Why Your Code Starts to Smell Bad - 2015.pdf:pdf},
isbn = {978-1-4799-1934-5},
mendeley-groups = {ANTI PATTERNS},
month = {May},
pages = {403--414},
publisher = {IEEE},
title = {{When and Why Your Code Starts to Smell Bad}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7194592},
year = {2015}
}

@article{xia1999proof,
title={A proof of the arithmetic mean-geometric mean-harmonic mean inequalities},
author={Xia, Da-Feng and Xu, Sen-Lin and Qi, Feng},
journal={RGMIA research report collection},
volume={2},
number={1},
year={1999},
publisher={School of Communications and Informatics, Faculty of Engineering and Science, Victoria University of Technology}
}

@article {turhan12,
author = {Turhan, Burak},
affiliation = {Department of Information Processing Science, University of Oulu, POB.3000, 90014 Oulu, Finland},
title = {On the dataset shift problem in software engineering prediction models},
journal = {Empirical Software Engineering},
publisher = {Springer Netherlands},
issn = {1382-3256},
keyword = {Computer Science},
pages = {62-74},
volume = {17},
issue = {1},
year = {2012}
}

@BOOK {Kerievsky2005,
author    = "Joshua Kerievsky",
title     = "Refactoring to Patterns",
publisher = "Addison-Wesly Professional",
year      = "2005"
}

@book{Lanza2006,
author = {Lanza, Michele and Marinescu, Radu},
mendeley-groups = {References},
pages = {207},
publisher = {Springer Verlag},
title = {{Object-Oriented Metrics in Practice: Using Software Metrics to Characterize, Evaluate, and Improve the Design of Object-Oriented Systems}},
year = {2006}
}

@misc{sq15,
title={Sonar{Q}ube: Open Source Quality Management},
author={A. Campbell},
year={2015},
note="Website: tiny.cc/2q4z9x"
}

@INPROCEEDINGS{Yamashita2013,
author={Yamashita, A. and Moonen, L.},
booktitle={Reverse Engineering (WCRE), 2013 20th Working Conference on},
title={Do developers care about code smells? An exploratory survey},
year={2013},
month={Oct},
pages={242-251},
keywords={program diagnostics;software maintenance;software quality;code decay;code quality;code smell detection;code smell removal;software maintenance problems;Encoding;Feature extraction;Java;Maintenance engineering;Programming;Software;code analysis tools;code smell detection;code smells;maintainability;refactoring;survey;usability},
doi={10.1109/WCRE.2013.6671299},}

@INPROCEEDINGS{Mantyla2004,
author={Mantyla, M.V. and Vanhanen, J. and Lassenius, C.},
booktitle={Software Maintenance, 2004. Proceedings. 20th IEEE International Conference on},
title={Bad smells - humans as code critics},
year={2004},
month={Sept},
pages={399-408},
keywords={program diagnostics;software houses;software metrics;software performance evaluation;Finnish software product company;bad code smell;code evaluation;code modules;poor software structures;source code metrics;subjective evaluation;Companies;Current measurement;Internet;Programming;Quality assessment;Software design;Software maintenance;Software measurement;Software quality;Software tools},
doi={10.1109/ICSM.2004.1357825},
ISSN={1063-6773},}

@ARTICLE{Sjoberg2013,
author={Sjoberg, D.I.K. and Yamashita, A. and Anda, B.C.D. and Mockus, A. and Dyba, T.},
journal={Software Engineering, IEEE Transactions on},
title={Quantifying the Effect of Code Smells on Maintenance Effort},
year={2013},
month={Aug},
volume={39},
number={8},
pages={1144-1156},
keywords={Java;regression analysis;software maintenance;Eclipse IDE plug-in;Java files;Java systems;code size reduction;code smell effect quantification;code smell refactoring;file properties;file size;maintainable code;maintenance effort;maintenance tasks;refused bequest;regression analysis;Context;Electronic mail;Java;Maintenance engineering;Software;Surgery;Time measurement;Maintainability;code churn;object-oriented design;product metrics},
doi={10.1109/TSE.2012.89},
ISSN={0098-5589},}



@ARTICLE{jorgensen09,
author =	 {J{\o}rgensen, Magne and Gruschke, Tanja M.},
journal =	 {Software Engineering, IEEE Transactions on},
title =	 {The Impact of Lessons-Learned Sessions on Effort
Estimation and Uncertainty Assessments},
year =	 2009,
month =	 {May-June },
volume =	 35,
number =	 3,
pages =	 {368 -383},
}

@inproceedings{passos11,
title =	 "Analyzing the Impact of Beliefs in Software Project
Practices",
author =	 "Carol Passos and Ana Paula Braun and Daniela
S. Cruzes and Manoel Mendonca",
year =	 2011,
booktitle =	 "ESEM'11"
}

@inproceedings {prem16,
title={Belief \& evidence in empirical software engineering},
author={Devanbu, Prem and Zimmermann, Thomas and Bird, Christian},
booktitle={Proceedings of the 38th International Conference on Software Engineering},
pages={108--119},
year={2016},
organization={ACM}
}

@inproceedings{zimmermann09,
author="T. Zimmermann and N.  Nagappan and H.  Gall and E.  Giger and B.  Murphy",
title="Cross-Project Defect Prediction",
booktitle="ESEC/FSE'09",
month="August",
year=2009
}

@inProceedings{me05a,
author    = {Tim Menzies and Zhihao Chen and Dan Port and Jairus Hihn},
title     = {Simple Software Cost Estimation: Safe or Unsafe?},
booktitle = {Proceedings, PROMISE workshop, ICSE 2005},
year      = 2005,
note     = {Available from \url{http://menzies.us/pdf/05safewhen.pdf}},
class    = "hW"
}

@article{jorg04,
author = {Magne Jorgensen},
title = {Realism in Assessment of Effort Estimation Uncertainty: It Matters How You Ask},
journal = {IEEE Trans. Softw. Eng.},
volume = {30},
number = {4},
year = {2004},
issn = {0098-5589},
pages = {209--217},
doi = {http://dx.doi.org/10.1109/TSE.2004.1274041},
publisher = {IEEE Press},
address = {Piscataway, NJ, USA},
}

@ARTICLE{Mendes2007,
author = {Kitchenham, Barbara and Mendes, Emilia and Travassos, Guilherme H.},
title = {Cross versus Within-Company Cost Estimation Studies: A Systematic
Review},
journal = {IEEE Trans. Softw. Eng.},
year = {2007},
volume = {33},
pages = {316--329},
number = {5},
note = {Member-Kitchenham, Barbara A.},
address = {Piscataway, NJ, USA},
doi = {http://dx.doi.org/10.1109/TSE.2007.1001},
issn = {0098-5589},
publisher = {IEEE Press}
}

@inproceedings{macdonell07,
 author = {MacDonell, Stephen G. and Shepperd, Martin J.},
 title = {Comparing Local and Global Software Effort Estimation Models -- Reflections on a Systematic Review},
 booktitle = {Proceedings of the First International Symposium on Empirical Software Engineering and Measurement},
 series = {ESEM '07},
 year = {2007},
 isbn = {0-7695-2886-4},
 pages = {401--409},
 numpages = {9},
 acmid = {1302959},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {D.2.9.b Cost estimation, project effort prediction, systematic review, empirical analysis.},
}

@INPROCEEDINGS{mair05,
author={Mair, C. and Shepperd, M.},
booktitle={Empirical Software Engineering, 2005. 2005 International Symposium on}, title={The consistency of empirical comparisons of regression and analogy-based software project cost prediction},
year={2005},
month={nov.},
volume={},
number={},
pages={ 10 pp.},
keywords={ analogy-based software project cost prediction; regression technique; software engineering cost estimation; project management; regression analysis; software cost estimation; software management;},
doi={10.1109/ISESE.2005.1541858},
ISSN={},}

@inproceedings{chawla03,
  title={C4. 5 and imbalanced data sets: investigating the effect of sampling method, probabilistic estimate, and decision tree structure},
  author={Chawla, Nitesh V},
  booktitle={Proceedings of the ICML},
  volume={3},
  year={2003}
}

@inproceedings{kubat97,
  title={Addressing the curse of imbalanced training sets: one-sided selection},
  author={Kubat, Miroslav and Matwin, Stan and others},
  booktitle={ICML},
  volume={97},
  pages={179--186},
  year={1997},
  organization={Nashville, USA}
}

@article{shatnawi10,
  title={A quantitative investigation of the acceptable risk levels of object-oriented metrics in open-source systems},
  author={Shatnawi, Raed},
  journal={IEEE Transactions on software engineering},
  volume={36},
  number={2},
  pages={216--225},
  year={2010},
  publisher={IEEE}
}

@article{shepperd2012evaluating,
  title={Evaluating prediction systems in software project estimation},
  author={Shepperd, Martin and MacDonell, Steve},
  journal={Information and Software Technology},
  volume={54},
  number={8},
  pages={820--827},
  year={2012},
  publisher={Elsevier}
}

@article{LANGDON201616,
title = "Exact Mean Absolute Error of Baseline Predictor, MARP0",
journal = "Information and Software Technology",
volume = "73",
number = "",
pages = "16 - 18",
year = "2016",
note = "",
issn = "0950-5849",
doi = "http://dx.doi.org/10.1016/j.infsof.2016.01.003",
url = "http://www.sciencedirect.com/science/article/pii/S0950584916000057",
author = "William B. Langdon and Javier Dolado and Federica Sarro and Mark Harman",
keywords = "Software engineering",
keywords = "Prediction systems",
keywords = "Empirical validation",
keywords = "Randomisation techniques",
keywords = "Search based software engineering"
}
@article{holte93,
    title = {{Very Simple Classification Rules Perform Well on Most Commonly Used Datasets}},
    year = {1993},
    journal = {Machine Learning},
    author = {Holte, R C},
    pages = {63},
    volume = {11}
}

@article{Whigham:2015,
 author = {Whigham, Peter A. and Owen, Caitlin A. and Macdonell, Stephen G.},
 title = {A Baseline Model for Software Effort Estimation},
 journal = {ACM Trans. Softw. Eng. Methodol.},
 issue_date = {May 2015},
 volume = {24},
 number = {3},
 month = may,
 year = {2015},
 issn = {1049-331X},
 pages = {20:1--20:11},
 articleno = {20},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/2738037},
 doi = {10.1145/2738037},
 acmid = {2738037},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Baseline model, transformed linear model},
}
@Article{shepperd12z,
  author =        {Martin J. Shepperd and Steven G. MacDonell},
  title =         {Evaluating prediction systems in software project estimation},
  journal =       {Information {\&} Software Technology},
  year =          {2012},
  volume =        {54},
  number =        {8},
  pages =         {820-827},
  date-added =    {2014-10-03 05:01:38 +0000},
  date-modified = {2014-10-03 05:01:38 +0000}
}

@article{palomba2015mining,
  title={Mining version histories for detecting code smells},
  author={Palomba, Fabio and Bavota, Gabriele and Di Penta, Massimiliano and Oliveto, Rocco and Poshyvanyk, Denys and De Lucia, Andrea},
  journal={IEEE Transactions on Software Engineering},
  volume={41},
  number={5},
  pages={462--489},
  year={2015},
  publisher={IEEE}
}

@inproceedings{palomba2016textual,
  title={A textual-based technique for smell detection},
  author={Palomba, Fabio and Panichella, Annibale and De Lucia, Andrea and Oliveto, Rocco and Zaidman, Andy},
  booktitle={Program Comprehension (ICPC), 2016 IEEE 24th International Conference on},
  pages={1--10},
  year={2016},
  organization={IEEE}
}

@article{morales2017use,
  title={On the use of developers’ context for automatic refactoring of software anti-patterns},
  author={Morales, Rodrigo and Soh, Z{\'e}phyrin and Khomh, Foutse and Antoniol, Giuliano and Chicano, Francisco},
  journal={Journal of Systems and Software},
  volume={128},
  pages={236--251},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{sae2016context,
  title={Context-based code smells prioritization for prefactoring},
  author={Sae-Lim, Natthawute and Hayashi, Shinpei and Saeki, Motoshi},
  booktitle={Program Comprehension (ICPC), 2016 IEEE 24th International Conference on},
  pages={1--10},
  year={2016},
  organization={IEEE}
}

@inproceedings{ducasse2004using,
  title={Using History Information to Improve Design Flaws Detection},
  author={Ducasse, St{\'e}phane and G{\^\i}rba, Tudor and Marinescu, Radu},
  booktitle={CSMR 2004: 8TH EUROPEAN CONFERENCE ON SOFTWARE MAINTENANCE AND REENGINEERING},
  year={2004},
  organization={Citeseer}
}

@article{ouni2013maintainability,
  title={Maintainability defects detection and correction: a multi-objective approach},
  author={Ouni, Ali and Kessentini, Marouane and Sahraoui, Houari and Boukadoum, Mounir},
  journal={Automated Software Engineering},
  pages={1--33},
  year={2013},
  publisher={Springer}
}
@article{kim2008classifying,
  title={Classifying software changes: Clean or buggy?},
  author={Kim, Sunghun and Whitehead Jr, E James and Zhang, Yi},
  journal={IEEE Transactions on Software Engineering},
  volume={34},
  number={2},
  pages={181--196},
  year={2008},
  publisher={IEEE}
}

@inproceedings{wang2016automatically,
  title={Automatically learning semantic features for defect prediction},
  author={Wang, Song and Liu, Taiyue and Tan, Lin},
  booktitle={Proceedings of the 38th International Conference on Software Engineering},
  pages={297--308},
  year={2016},
  organization={ACM}
}
@article{Wo97,
 author = {Wolpert, D. H. and Macready, W. G.},
 title = {No Free Lunch Theorems for Optimization},
 journal = {Trans. Evol. Comp},
 issue_date = {April 1997},
 volume = {1},
 number = {1},
 month = apr,
 year = {1997},
 issn = {1089-778X},
 pages = {67--82},
 numpages = {16},
 url = {http://dx.doi.org/10.1109/4235.585893},
 doi = {10.1109/4235.585893},
 acmid = {2221408},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}
@book{cohen95,
    title = {{Empirical Methods for Artificial Intelligence}},
    year = {1995},
    author = {Cohen, P R},
    publisher = {MIT Press}
}
