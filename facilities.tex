%\documentclass[10pt]{article}
%\usepackage{graphicx}
%\usepackage{subfigure}
%\usepackage{times}
%\usepackage{cite}
%\usepackage{subfigure}
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{mscp}
%\usepackage{url}

%\begin{document}

\pagestyle{empty}
\setcounter{page}{1}
\setcounter{section}{0}
\setcounter{page}{1}
\setcounter{section}{0}

% \head{Facilities, Equipment, and Other Resources:}
NCSU completed the Engineering Building II on its Centennial Campus in
January 2006. The new building now houses the Computer Science
Department with its laboratories, which are accessible to the proposed
project. In particular, one laboratory will be devoted to the research
tasks of this proposal.

\paragraph{Facilities within NCS CS Department:}
The department has a 108-node compute cluster named ARC with about
2,000 cores (AMD Mangy-Cores), Infiniband QDR interconnect, per node
power monitoring, GPUs and SSDs and parallel file system support,
which was funded by an NSF CRI that he is the main PI of together with
5 co-PIs. 
%
The ARC facility is providing local and remote researchers with
administrator/root privileges for Computer Science experiments at
medium scale. This allows any of the software layers, including the
operating system and Infiniband switch network routing tables, to be
modified for experimental purposes, e.g., to experiment with different
network topologies.  For large-scale demonstrations, remote 
facilities will be utilized (see below).

%This platform can be used for software development of integrated
%architectures. We plan to experiment on multiple Intel Core i7 x86
%CPU/GPU platforms, AMD APUs, and 2 NVIDIA Tegra TK1 development kits
%in our lab. We also we plan to acquire additional integrated platforms
%as part of this grant, including an Intel Xeon Phi (Knight's Landing).

% \paragraph{Remote Computing Resources:}
% The PI has access to the GPU cluster and Infiniband cluster at the
% Thomas Jefferson National Accelerator Facility of the U.S. Department
% of Energy. The GPU cluster includes 117 Intel Nahalem nodes, equipped
% with over 200 Tesla S1070 in total and connected with QDR or SDR
% Ininiband. The Infiniband cluster contains three sub-clusters. One
% contains 224 nodes, each equipped with dual quad-core 2.53 GHz
% Westmere CPUs, 24 GB memory, and QDR (40 Gb/s) Infiniband; one
% contains 320 nodes, each equipped with dual quad-core 2.4 GHz Nehalem
% CPUs, 24 GB memory, and QDR (40 Gb/s) Infiniband; another contains 392
% nodes, each quipped with dual quad-core 1.9 GHz Opteron CPUs, 8 GB
% memory, and DDR (20 Gb/s) Infiniband.

\paragraph{Computing Resources:}
%% RECHECK
The College of Engineering at North Carolina State University has built
a distributed computing environment named ``Eos'' for engineering education.
The Eos environment consists of more than 1,000 public and private
workstations and supports more than 12,000 users campus wide.
The success of Eos in the College of Engineering has spawned similar
projects in other colleges and in the campus computing center. Recently,
these projects have merged into a single distributed computing system
supporting more than 30,000 faculty, staff, and students. This computing
environment not only serves the academic computing needs of the campus,
but is also becoming the primary means of communication between the students
and the faculty. A large number of software packages are available on the
Eos system.

Additionally, NC State University provides a High-Performance Computing (HPC) facility as a part of the initiative to provide state of the art support for research and academic computing. HPC system (called henry2) provides NC State students and faculty with entry and medium level high-performance research and education computing facilities, consulting support and scientific workflow support. The HPC ecosystem consists of 1233 dual Xeon compute nodes in the henry2 cluster. Each node has two Xeon processors (mix of dual-, quad-, six-, eight-, ten-core, twelve-core) and 2 to 6 GigaBytes of memory per core. The total number of cores increases as more cores are purchased and now exceeds 10000. The nodes all have 64-bit processors. All HPC projects have capability to run jobs using up to 128 processor cores up to 48 hours and smaller jobs up to a week.

\paragraph{Office:}
The PI has an
office in the CS Department.  The Engineering Building II
has adequate space to house all research assistants
working on this project. All offices are wired for high-speed network
access.

\paragraph{Other Resources:}
The departments provide the space and basic networking services to
carry out the experiments, secretarial and administrative support as
well as general-purpose office equipment ({\em e.g.}, fax, photocopiers,
etc.).


